# ğŸ”¥ Awesome Robust Driving World Models 

This repository focuses on **driving world models (DWM)** with an emphasis on their **task taxonomy** and **progressive robustness (Robustness 1.0 â†’ 3.0)**, base on the survey:

[[**Progressive Robustness-Aware World Models in Autonomous Driving: A Review and Outlook**](https://doi.org/10.36227/techrxiv.176523308.84756413/v1)]

## Citation

If you find this repository or the survey useful, please consider â­ this repo and citing the paper.

```bibtex
@article{jia2025progressive,
  title   = {{Progressive Robustness-Aware World Models in Autonomous Driving: A Review and Outlook}},
  author  = {Feiyang Jia and Caiyan Jia and Ziying Song and Zhicheng Bao and Lin Liu and Shaoqing Xu and Yan Gong and Lei Yang and Xinyu Zhang and Bin Sun and Xiaoshuai Hao and Long Chen and Yadan Luo},
  journal = {TechRxiv},
  year    = {2025},
  note    = {preprint},
  doi     = {10.36227/techrxiv.176523308.84756413/v1}
}
```

## Paper Recommendations 

If youâ€™d like to suggest something, please open an new ISSUE page and (if possible) provide:

1. **Online link** to the paper / project homepage / code repository. 
2. The suggested **category**, following this repo:
   - Task: Generation / Planning / Enhancement
   - Robustness level: Robustness 1.0 / 2.0 / 3.0

---

# ğŸ“Œ Overview of Contents

- ğŸ“„ 1. Summary: Information, Task, and Robustness Level
- ğŸ›¡ï¸ 2. Progressive Robustness Analysis: 1.0, 2.0 and 3.0
  - 2.1 Robustness 1.0 â€“ Self-Metrics & Evaluation Protocols
  - 2.2 Robustness 2.0 â€“ Contributions to Autonomous Driving Systems
  - 2.3 Robustness 3.0 â€“ Open-World Robustness & Future Directions

---

# ğŸ“„ 1. Summary: Information, Task, and Robustness Level

| Abbr.      | Pub.     | Full Title | Paper  | Page | Code | Gene. | Plan. | Enh. | Lv. |
|:-----------------:|:---------:|:-----------|:------:|:----:|:----:|:----------:|:--------:|:-----------:|:-----------------:|
| **Iso-Dream** | NIPS22 | Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models | [[âœ“](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9316769afaaeeaad42a9e3633b14e801-Abstract-Conference.html)] | âœ— | [[âœ“](https://github.com/panmt/Iso-Dream)] | âœ“ | âœ“ | âœ— | 2.0 |
| **SEM2** | NIPS22 | Model-based imitation learning for urban driving | [[âœ“](https://arxiv.org/abs/2210.04017)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **MILE** | NIPS22 | SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model | [[âœ“](https://proceedings.neurips.cc/paper_files/paper/2022/hash/827cb489449ea216e4a257c47e407d18-Abstract-Conference.html)] | âœ— | [[âœ“](https://github.com/wayveai/mile)] | âœ“ | âœ“ | âœ— | 2.0 |
| **ADriver-I** | arXiv23 | ADriver-I: A General World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2311.13549)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **GAIA-1** | arXiv23 | GAIA-1: A Generative World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2309.17080)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **UniWorld** | arXiv23 | UniWorld: Autonomous Driving Pre-training via World Models | [[âœ“](https://arxiv.org/abs/2308.07234)] | âœ— | [[âœ“](https://github.com/chaytonmin/UniWorld)] | âœ“ | âœ— | âœ“ | 2.0 |
| **TrafficBots** | ICRA23 | TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction | [[âœ“](https://arxiv.org/abs/2303.04116)] | âœ— | [[âœ“](https://github.com/SysCV/TrafficBots)] | âœ“ | âœ“ | âœ— | 2.0 |
|   **DrivingWorld**   | arXiv24 | DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT                                         | [[âœ“](https://arxiv.org/abs/2412.19505)] |                                      âœ—                                      |                          [[âœ“](https://github.com/YvanYin/DrivingWorld)]                          |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|  **InfinityDrive**  | arXiv24 | InfinityDrive: Breaking Time Limits in Driving World Models                                                         | [[âœ“](https://arxiv.org/abs/2412.01522)] | [[âœ“](https://metadrivescape.github.io/papers_project/InfinityDrive/page.html)] |                                               âœ—                                               |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|      **GenAD**      |   CVPR24   | Generalized Predictive Model for Autonomous Driving                                                                 | [[âœ“](https://arxiv.org/abs/2403.09630)] |                                      âœ—                                      | [[dataset](https://github.com/OpenDriveLab/DriveAGI?tab=readme-ov-file#genad-dataset-opendv-youtube)] |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|      **TERRA**      | arXiv24 | Towards Action Controllable World Models for Autonomous Driving                                                     | [[âœ“](https://arxiv.org/abs/2412.05337)] |                [[âœ“](https://turingmotors.github.io/actbench/)]                |                         [[âœ“](https://github.com/turingmotors/ACT-Bench)]                         |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|      **Vista**      | NIPS24 | Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability                         | [[âœ“](https://arxiv.org/abs/2405.17398)] |                                      âœ—                                      |                           [[âœ“](https://github.com/OpenDriveLab/Vista)]                           |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|  **DINO-Foresight**  | arXiv24 | DINO-Foresight: Self-Supervised Semantic Foresight for Autonomous Driving                                           | [[âœ“](https://arxiv.org/abs/2412.11673)] |                                      âœ—                                      |                          [[âœ“](https://github.com/Sta8is/DINO-Foresight)]                          |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|   **DriveGenVLM**   |  IAVVC24  | DriveGenVLM: Real-world Video Generation for Autonomous Driving with Vision Language Models                         | [[âœ“](https://arxiv.org/abs/2408.16647)] |                                      âœ—                                      |                                               âœ—                                               |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|      **Doe-1**      | arXiv24 | Doe-1: Driving on Earth with One Transformer                                                                        | [[âœ“](https://arxiv.org/abs/2412.09627)] |                                      âœ—                                      |                               [[âœ“](https://github.com/wzzheng/Doe)]                               |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|     **UniMLVG**     | arXiv24 | UniMLVG: Unified Multi-View LiDAR-Video Generation for Autonomous Driving                                           | [[âœ“](https://arxiv.org/abs/2412.09628)] |                [[âœ“](https://sensetime-fvg.github.io/UniMLVG/)]                |                          [[âœ“](https://github.com/SenseTime-FVG/OpenDWM)]                          |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|     **Drive-WM**     |   CVPR24   | Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving          | [[âœ“](https://arxiv.org/abs/2311.17918)] |                       [[âœ“](https://drive-wm.github.io/)]                       |                           [[âœ“](https://github.com/BraveGroup/Drive-WM)]                           |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|   **DriveDreamer**   |   ECCV24   | DriveDreamer: Towards Real-world-driven Generative World Models for Autonomous Driving                              | [[âœ“](https://arxiv.org/abs/2309.09777)] |                     [[âœ“](https://drivedreamer.github.io/)]                     |                        [[âœ“](https://github.com/JeffWang987/DriveDreamer)]                        |  âœ“  |  âœ“  |  âœ“  | 2.0 |
| **DrivingDiffusion** |   ECCV24   | Layout-Guided multi-view driving scene video generation with latent diffusion model                                 | [[âœ“](https://arxiv.org/abs/2310.07771)] |                   [[âœ“](https://drivingdiffusion.github.io/)]                   |                        [[âœ“](https://github.com/shalfun/DrivingDiffusion)]                        |  âœ“  |  âœ—  |  âœ“  | 2.0 |
|   **DrivePhysica**   | arXiv24.12 | DrivePhysica: Physical-Consistent Video Generation for Autonomous Driving                                           | [[âœ“](https://arxiv.org/abs/2412.09621)] |                                      âœ—                                      |           [[âœ“](https://metadrivescape.github.io/papers_project/DrivePhysica/page.html)]           |  âœ“  |  âœ—  |  âœ“  | 2.0 |
|     **Panacea**     |   CVPR24   | Panoramic and Controllable Video Generation for Autonomous Driving                                                  | [[âœ“](https://arxiv.org/abs/2311.16813)] |                      [[âœ“](https://panacea-ad.github.io/)]                      |                            [[âœ“](https://github.com/wenyuqing/panacea)]                            |  âœ“  |  âœ—  |  âœ“  | 2.0 |
|    **DriveScape**    | arXiv24.09 | DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation                                | [[âœ“](https://arxiv.org/abs/2409.05463)] |                                      âœ—                                      |                                               âœ—                                               |  âœ“  |  âœ—  |  âœ“  | 2.0 |
|    **HoloDrive**    | arXiv24.12 | Holistic 2D-3D Multi-Modal Street Scene Generation for Autonomous Driving                                                      | [[âœ“](https://arxiv.org/abs/2412.01407)] |                                      âœ—                                      |                                               âœ—                                               |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|     **WoVoGen**     |   ECCV24   | World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation                                                   | [[âœ“](https://arxiv.org/abs/2312.02934)] |                      âœ—                       |             [[âœ“](https://github.com/fudan-zvg/WoVoGen)]                                                                                  |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|    **Copilot4D**    |   ICLR24   | Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion                                                  | [[âœ“](https://arxiv.org/abs/2311.01017)] |                                      âœ—                                      |                     âœ—                          |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|  **DFIT-OccWorld**  | arXiv24 | An Efficient Occupancy World Model via Decoupled Dynamic Flow and Image-assisted Training                                                             | [[âœ“](https://arxiv.org/abs/2412.13772)] |                                      âœ—                                      |                                               âœ—                                               |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|      **ViDAR**      |   CVPR24   | Visual Point Cloud Forecasting enables Scalable Autonomous Driving                                                                  | [[âœ“](https://arxiv.org/abs/2312.17655)] |                                      âœ—                                      |                           [[âœ“](https://github.com/OpenDriveLab/ViDAR)]                           |  âœ“  |  âœ“  |  âœ“  | 2.0 |
|       **UnO**       |   CVPR24   | Unsupervised Occupancy Fields for Perception and Forecasting                                                  | [[âœ“](https://arxiv.org/abs/2406.08691)] |                                       [[âœ“](https://waabi.ai/research/uno)]                                      |                             [[âœ“](https://waabi.ai/research/uno)]                              |  âœ“  |  âœ—  |  âœ“  | 2.0 |
|     **OccWorld**     |   ECCV24   | OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving                                                | [[âœ“](https://arxiv.org/abs/2311.16038)] |                      [[âœ“](https://wzzheng.net/OccWorld)]                      |                            [[âœ“](https://github.com/wzzheng/OccWorld)]                            |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|       **DOME**       | arXiv24 | Taming Diffusion Model into High-Fidelity Controllable Occupancy World Model                                              | [[âœ“](https://arxiv.org/abs/2410.10429)] |                     [[âœ“](https://gusongen.github.io/DOME)]                     |                 [[âœ“](https://github.com/gusongen/DOME)]                                                                                |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|    **DriveWorld**    |   CVPR24   | 4D Pre-trained Scene Understanding via World Models for Autonomous Driving                                                                  | [[âœ“](https://arxiv.org/abs/2405.04390)] |                                      âœ—                                      |                       âœ—                            |  âœ“  |  âœ“  |  âœ“  | 2.0 |
|     **Cam4DOCC**     |   CVPR24   | Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications                     | [[âœ“](https://arxiv.org/abs/2311.17663)] |                            âœ—            |                            [[âœ“](https://github.com/haomo-ai/Cam4DOcc)]                            |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|     **OccSora**     | arXiv24 | OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving                                  | [[âœ“](https://arxiv.org/abs/2405.17833)] |                                      âœ—                                      |                             [[âœ“](https://github.com/haomo-ai/Cam4DOcc)]                             |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|       **NeMo**       |   ECCV24   | Neural Volumetric World Models for Autonomous Driving                                                                                 | [[âœ“](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02571.pdf)] |                                      âœ—                                      |                            âœ—                            |  âœ“  |  âœ“  |  âœ“  | 2.0 |
|     **OccLLaMA**     | arXiv24 | An Occupancy-Language-Action Generative World Model for Autonomous Driving                                                       | [[âœ“](https://arxiv.org/abs/2409.03272)] |                                      âœ—                                      |                            âœ—                            |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|       **LAW**       | arXiv24 | Enhancing End-to-end Autonomous Driving with Latent World Model                                                     | [[âœ“](https://arxiv.org/abs/2406.08481)] |                                      âœ—                                      |                             [[âœ“](https://github.com/BraveGroup/LAW)]                             |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|    **CarFormer**    |   ECCV24   | CarFormer: Self-Driving with Learned Object-Centric Representations                                                 | [[âœ“](https://arxiv.org/abs/2407.15843)] |                  [[âœ“](https://kuis-ai.github.io/CarFormer/)]                  |                            [[âœ“](https://github.com/Shamdan17/CarFormer)]                            |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|      **GenAD**      |   ECCV24   | Generative End-to-End Autonomous Driving                                                                     | [[âœ“](https://arxiv.org/abs/2402.11502)] |                     âœ—                      |                              [[âœ“](https://github.com/wzzheng/GenAD)]                              |  âœ“  |  âœ“  |  âœ“  | 2.0 |
|  **SceneDiffuser**  | NIPS24 | Efficient and Controllable Driving Simulation Initialization and Rollout                                                   | [[âœ“](https://arxiv.org/abs/2412.12129)] |                                      âœ—                                      |                         âœ—                     |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|     **MARL-CCE**     |   ECCV24   | Modelling Competitive Behaviors in Autonomous Driving Under Generative World Model      | [[âœ“](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05085.pdf)] |                                      âœ—                                      |                          [[âœ“](https://github.com/qiaoguanren/MARL-CCE)]                          |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|      **RAMBLE**      | arXiv24 | From Imitation to Exploration: End-to-end Autonomous Driving based on World Model                                                               | [[âœ“](https://arxiv.org/abs/2410.02253)] |           âœ—           |                              âœ—                              |  âœ“  |  âœ“  |  âœ—  | 2.0 |
| **Imagine-2-Drive** | arXiv24 | High-Fidelity World Modeling in CARLA for Autonomous Vehicles                                                       | [[âœ“](https://arxiv.org/abs/2411.10171)] |                                   [[âœ“](https://anantagrg.github.io/Imagine-2-Drive.github.io/)]                                         |                                               âœ—                                               |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|   -   | arXiv24 | Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models | [[âœ“](https://arxiv.org/abs/2409.16663)] |                                      âœ—                                      |                                               âœ—                                               |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|   **Think2Drive**   |   ECCV24   | Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving      | [[âœ“](https://arxiv.org/abs/2402.16720)] |                 âœ—                 |                          âœ—                        |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|       **GUMP**       |   ECCV24   | Solving Motion Planning Tasks with a Scalable Generative Model                                                                    | [[âœ“](https://arxiv.org/abs/2407.02797)] |                  âœ—                  |                               [[âœ“](https://github.com/HorizonRobotics/GUMP/)]                               |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|    **Dreamland**    |  arXiv25  | Dreamland: Controllable World Creation with Simulator and Generative Models                           |            [[âœ“](https://arxiv.org/abs/2506.08006)]            |   [[âœ“](https://metadriverse.github.io/dreamland/)]   |                          âœ—                          |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|      **Orbis**      |  arXiv25  | Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models                       |            [[âœ“](https://arxiv.org/abs/2507.13162)]            |                         âœ—                         |      [[âœ“](https://github.com/lmb-freiburg/Orbis)]      |  âœ“  |  âœ—  |  âœ—  | 2.0 |
| **STAGE** | IROS25 | STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation | [[âœ“](https://arxiv.org/abs/2506.13138)] | [[âœ“](https://4dvlab.github.io/STAGE/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **DriVerse** | ACM MM25 | DriVerse: Navigation World Model for Driving Simulation via Multimodal Trajectory Prompting and Motion Alignment | [[âœ“](https://arxiv.org/abs/2504.18576)] | âœ— | [[âœ“](https://github.com/shalfun/DriVerse)] | âœ“ | âœ— | âœ— | 2.0 |
|  **ReconDreamer**  |  CVPR25  | ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration           |            [[âœ“](https://arxiv.org/abs/2411.19548)]            |                         âœ—                         | [[âœ“](https://github.com/GigaAI-research/ReconDreamer)] |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|   **ProphetDWM**   |  arXiv25  | ProphetDWM: A Driving World Model for Rolling Out Future Actions and Videos                           |            [[âœ“](https://arxiv.org/abs/2505.18650)]            |                         âœ—                         |                          âœ—                          |  âœ“  |  âœ—  |  âœ—  | 2.0 |
|     **FSDrive**     | NeurIPS25 | FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving                   |            [[âœ“](https://arxiv.org/abs/2505.17685)]            |                         âœ—                         |       [[âœ“](https://github.com/MIV-XJTU/FSDrive)]       |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|   **DrivingGPT**   |  ICCV25  | DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers |            [[âœ“](https://arxiv.org/abs/2412.18607)]            |    [[âœ“](https://rogerchern.github.io/DrivingGPT/)]    |                          âœ—                          |  âœ“  |  âœ“  |  âœ—  | 2.0 |
| **GeoDrive** | arXiv25 | GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control | [[âœ“](https://arxiv.org/abs/2505.22421)] | âœ— | [[âœ“](https://github.com/antonioo-c/GeoDrive)] | âœ“ | âœ“ | âœ— | 2.0 |
|      **Epona**      |  ICCV25  | Epona: Autoregressive Diffusion World Model for Autonomous                                            |            [[âœ“](https://arxiv.org/abs/2506.24113)]            |       [[âœ“](https://kevin-thu.github.io/Epona/)]       |        [[âœ“](https://github.com/Kevin-thu/Epona)]        |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|   **ImagiDrive**   |  arXiv25  | ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving                       |            [[âœ“](https://arxiv.org/abs/2508.11428)]            |                         âœ—                         |     [[âœ“](https://github.com/fudan-zvg/ImagiDrive)]     |  âœ“  |  âœ“  |  âœ—  | 2.0 |
|      **ReSim**      |  arXiv25  | ReSim: Reliable World Simulation for Autonomous Driving                                               |            [[âœ“](https://arxiv.org/abs/2506.09981)]            |         [[âœ“](https://opendrivelab.com/ReSim)]         |      [[âœ“](https://github.com/OpenDriveLab/ReSim)]      |  âœ“  |  âœ“  |  âœ—  | 2.0 |
| **VaViM/VaVAM** | arXiv25 | VaViM and VaVAM: Autonomous Driving through Video Generative Modeling | [[âœ“](https://arxiv.org/abs/2502.15672)] | [[âœ“](https://valeoai.github.io/vavim-vavam/)] | [[âœ“](https://github.com/valeoai/VideoActionModel)] | âœ“ | âœ“ | âœ“ | 2.0 |
| **DriveDreamer4D** | CVPR25 | DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation | [[âœ“](https://arxiv.org/abs/2410.13571)] | [[âœ“](https://drivedreamer4d.github.io/)] | [[âœ“](https://github.com/GigaAI-research/DriveDreamer4D)] | âœ“ | âœ“ | âœ— | 2.0 |
| **Drive&Gen** | IROS25 | Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models | [[âœ“](https://arxiv.org/abs/2510.06209)] | âœ— | âœ— | âœ“ | âœ“ | âœ“ | 2.0 |
| **SimWorld** | arXiv25 | A Unified Benchmark for Simulator-Conditioned Scene Generation via World Model | [[âœ“](https://arxiv.org/abs/2503.13952)] | âœ— | [[âœ“](https://github.com/Li-Zn-H/SimWorld)] | âœ“ | âœ— | âœ“ | 2.0 |
| **UMGen** | CVPR25 | Generating Multimodal Driving Scenes via Next-Scene Prediction | [[âœ“](https://arxiv.org/abs/2503.14945)] | [[âœ“](https://yanhaowu.github.io/UMGen/)] | [[âœ“](https://github.com/YanhaoWu/UMGen/)] | âœ“ | âœ— | âœ— | 2.0 |
| **InfiniCube** | ICCV25 | Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models | [[âœ“](https://arxiv.org/abs/2412.03934)] | [[âœ“](https://research.nvidia.com/labs/toronto-ai/infinicube/)] | [[âœ“](https://github.com/nv-tlabs/InfiniCube)] | âœ“ | âœ— | âœ— | 2.0 |
| **GEM** | CVPR25 | A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control | [[âœ“](https://arxiv.org/abs/2412.11198)] | [[âœ“](https://vita-epfl.github.io/GEM.github.io/)] | [[âœ“](https://github.com/vita-epfl/GEM)] | âœ“ | âœ“ | âœ— | 2.0 |
| **T^3Former** | arXiv25 | Temporal Triplane Transformers as Occupancy World Models | [[âœ“](https://arxiv.org/abs/2503.07338)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **MUVO** | IEEE25 | MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations | [[âœ“](https://arxiv.org/abs/2311.11762)] | âœ— | âœ— | âœ“ | âœ— | âœ“ | 2.0 |
| **UniFuture** | arXiv25 | Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception | [[âœ“](https://arxiv.org/abs/2503.13587)] | [[âœ“](https://dk-liang.github.io/UniFuture/)] | [[âœ“](https://github.com/dk-liang/UniFuture)] | âœ“ | âœ— | âœ— | 2.0 |
| **Cosmos-7B** | arXiv25 | Scalable Synthetic Driving Data Generation with World Foundation Models | [[âœ“](https://arxiv.org/abs/2506.09042)] | [[âœ“](https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams/)] | [[âœ“](https://github.com/nv-tlabs/Cosmos-Drive-Dreams)] | âœ“ | âœ— | âœ“ | 2.0 |
| **MaskGWM** | CVPR25 | MaskGWM: Masked Generative World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2500.00000)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **DriveDreamer-2** | AAAI25 | DriveDreamer-2: LLM-Enhanced World Models for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2403.06845)] | [[âœ“](https://drivedreamer-2.github.io/)] | [[âœ“](https://github.com/DriveDreamer/DriveDreamer-2)] | âœ“ | âœ— | âœ“ | 2.0 |
| **MiLA** | arXiv25 | MiLA: Multi-View Long-Horizon Autonomous Driving Video Generation | [[âœ“](https://arxiv.org/abs/2503.15875)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **LongDWM** | arXiv25 | Cross-Granularity Distillation for Building a Long-Term Driving World Model | [[âœ“](https://arxiv.org/abs/2506.01546)] | [[âœ“](https://wang-xiaodong1899.github.io/longdwm/)] | [[âœ“](https://github.com/Wang-Xiaodong1899/Long-DWM)] | âœ“ | âœ— | âœ— | 2.0 |
| **GAIA-2** | arXiv25 | GAIA-2: A Generalist Generative World Model for Autonomous Driving (Wayve) | [[âœ“](https://arxiv.org/abs/2503.20523)] | [[âœ“](https://wayve.ai/thinking/gaia-2/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **MoVieDrive** | arXiv25 | Multi-Modal Multi-View Urban Scene Video Generation | [[âœ“](https://arxiv.org/abs/2508.14327)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **DrivingSphere** | CVPR25 | DrivingSphere: Building a High-fidelity 4D World for Closed-loop Simulation | [[âœ“](https://arxiv.org/abs/2411.11252)] | [[âœ“](https://yanty123.github.io/DrivingSphere/)] | [[âœ“](https://github.com/yanty123/DrivingSphere)] | âœ“ | âœ“ | âœ— | 2.0 |
| **Glad** | ICLR25 | Glad: A Streaming Scene Generator for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2503.00045)] | âœ— | [[âœ“](https://github.com/xb534/Glad)] | âœ“ | âœ— | âœ“ | 2.0 |
| **DiVE** | ICLR25 | Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer | [[âœ“](https://arxiv.org/abs/2504.19614)] | âœ— | âœ— | âœ“ | âœ— | âœ“ | 2.0 |
| **CVD-STORM** | arXiv25 | Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2510.07944)] | âœ— | âœ— | âœ“ | âœ— | âœ“ | 2.0 |
| **DiST-4D** | ICCV25 | 4D Driving Scene Generation With Stereo Forcing | [[âœ“](https://arxiv.org/abs/2509.20251)] | [[âœ“](https://royalmelon0505.github.io/DiST-4D/)] | [[âœ“](https://github.com/royalmelon0505/dist4d)] | âœ“ | âœ— | âœ— | 2.0 |
| **PhiGensis** | arXiv25 | 4D Driving Scene Generation With Stereo Forcing | [[âœ“](https://arxiv.org/abs/2509.20251)] | [[âœ“](https://jiangxb98.github.io/PhiGensis/)] | [[âœ“](https://github.com/LuPaoPao/PhiGensis)] | âœ“ | âœ“ | âœ“ | 2.0 |
| **EOT-WM** | arXiv25 | Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latant Space | [[âœ“](https://arxiv.org/abs/2503.09215)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **BEVWorld--** | ICLR25 | BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space | [[âœ“](https://arxiv.org/abs/2407.05679)] | âœ— | [[âœ“](https://github.com/zympsyche/BevWorld)] | âœ“ | âœ“ | âœ“ | 2.0 |
| **UniScene** | CVPR25 | Unified Occupancy-centric Driving Scene Generation | [[âœ“](https://arxiv.org/abs/2412.05435)] | [[âœ“](https://arlo0o.github.io/uniscene/)] | [[âœ“](https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation)] | âœ“ | âœ— | âœ“ | 2.0 |
| **Ao et al.** | ICCVW25 | â€” | [[âœ“](https://arxiv.org/abs/2509.11959)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **LiDARCrafter** | arXiv25 | Dynamic 4D World Modeling from LiDAR Sequences | [[âœ“](https://arxiv.org/abs/2508.03692)] | [[âœ“](https://lidarcrafter.github.io/)] | [[âœ“](https://github.com/worldbench/LiDARCrafter)] | âœ“ | âœ“ | âœ— | 2.0 |
| **LidarDM** | ICRA25 | LidarDM: LiDAR Diffusion Model | [[âœ“](https://arxiv.org/abs/2409.07463)] | âœ— | âœ— | âœ“ | âœ— | âœ“ | 2.0 |
| **HERMES** | ICCV25 | A Unified Self-Driving World Model | [[âœ“](https://arxiv.org/abs/2505.16394)] | [[âœ“](https://lmd0311.github.io/HERMES/)] | [[âœ“](https://github.com/LMD0311/HERMES)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DriveX** | arXiv25 | Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving | [[âœ“](https://arxiv.org/abs/2505.19239)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **DIO** | CVPR25 | Decomposable Implicit 4D Occupancy-Flow World Model | [[âœ“](https://openaccess.thecvf.com/content/CVPR2025/html/Diehl_DIO_Decomposable_Implicit_4D_Occupancy-Flow_World_Model_CVPR_2025_paper.html)] | âœ— | âœ— | âœ“ | âœ— | âœ“ | 2.0 |
| **GaussianWorld** | CVPR25 | Gaussian World Model for Streaming 3D Occupancy Prediction | [[âœ“](https://arxiv.org/abs/2412.10373)] | âœ— | [[âœ“](https://github.com/zuosc19/GaussianWorld)] | âœ“ | âœ— | âœ— | 2.0 |
| **DynamicCity** | ICLR25 | Large-Scale 4D Occupancy Generation from Dynamic Scenes | [[âœ“](https://arxiv.org/abs/2410.18084)] | âœ— | [[âœ“](https://github.com/3DTopia/DynamicCity)] | âœ“ | âœ— | âœ— | 2.0 |
| **OccProphet** | ICLR25 | Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework | [[âœ“](https://arxiv.org/abs/2502.15180)] | âœ— | [[âœ“](https://github.com/JLChen-C/OccProphet)] | âœ“ | âœ— | âœ— | 2.0 |
| **Tianran L.et. al** | arXiv25 | Towards Foundational LiDAR World Models with Efficient Latent Flow Matching | [[âœ“](https://arxiv.org/abs/2506.23434)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **PreWorld** | ICLR25 | Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2502.07309)] | âœ— | [[âœ“](https://github.com/getterupper/PreWorld)] | âœ“ | âœ“ | âœ— | 2.0 |
| **Drive-OccWorld** | AAAI25 |  Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2408.14197)] | [[âœ“](https://drive-occworld.github.io/)] | [[âœ“](https://github.com/yuyang-cloud/Drive-OccWorld)] | âœ“ | âœ“ | âœ— | 2.0 |
| **OccTENS** | arXiv25 | 3D Occupancy World Model via Temporal Next-Scale Prediction | [[âœ“](https://arxiv.org/abs/2509.03887)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **RenderWorld** | ICRA25 | RenderWorld: World Model with Self-Supervised 3D Label | [[âœ“](https://arxiv.org/abs/2409.11356)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **I2-World** | arXiv25 | Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting | [[âœ“](https://arxiv.org/abs/2507.09144)] | âœ— | [[âœ“](https://github.com/lzzzzzm/II-World)] | âœ“ | âœ— | âœ“ | 2.0 |
| **Occ-LLM** | ICRA25 | Occ-LLM: Enhancing Autonomous Driving with Occupancy-Based Large Language Models | [[âœ“](https://arxiv.org/abs/2502.06419)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **CTT** | IEEE25 | Categorical Traffic Transformer: Interpretable and Diverse Behavior Prediction with Tokenized Latent | [[âœ“](https://arxiv.org/abs/2311.18307)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **PIWM** | TIV25 | Dream to Drive with Predictive Individual World Model | [[âœ“](https://arxiv.org/abs/2501.16733)] | âœ— | [[âœ“](https://github.com/gaoyinfeng/PIWM)] | âœ“ | âœ“ | âœ— | 2.0 |
| **Dream to Drive** | arXiv25 | Dream to Drive: Model-Based Vehicle Control Using Analytic World Models | [[âœ“](https://arxiv.org/abs/2502.10012)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **WoTE** | ICCV25 | End-to-End Driving with Online Trajectory Evaluation via BEV World Model | [[âœ“](https://arxiv.org/abs/2503.22231)] | âœ— | [[âœ“](https://github.com/liyingyanUCAS/WoTE)] | âœ“ | âœ“ | âœ— | 2.0 |
| **SceneDiffuser++** | CVPR25 | SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model | [[âœ“](https://openaccess.thecvf.com/content/CVPR2025/html/Tan_SceneDiffuser_City-Scale_Traffic_Simulation_via_a_Generative_World_Model_CVPR_2025_paper.html)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **AdaptiveDriver** | ICRA25 | Planning with Adaptive World Models for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2406.10714)] | [[âœ“](https://arunbalajeev.github.io/world_models_planning/world_model_paper.html)] | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **LatentDriver** | ICRA25 | Learning Multiple Probabilistic Decisions from Latent World Model in Autonomous Driving | [[âœ“](https://arxiv.org/abs/2409.15730)] | âœ— | [[âœ“](https://github.com/Sephirex-X/LatentDriver)] | âœ“ | âœ“ | âœ— | 2.0 |
| **Safedrive Dreamer** | AEJ25 | Safedrive dreamer: Navigating safetyâ€“critical scenarios in autonomous driving with world models | [[âœ“]()] | âœ— | âœ— | âœ“ | âœ— | âœ“ | 2.0 |
| **World4Drive** | ICCV25 | World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model | [[âœ“](https://arxiv.org/abs/2507.00603)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **WcDT** | ICRA25 | End-to-End Driving with Online Trajectory Evaluation via BEV World Model | [[âœ“]()] | âœ— | [[âœ“](https://arxiv.org/abs/2504.01941)] | âœ“ | âœ“ | âœ— | 2.0 |
| **FASTopoWM** | arXiv25 | FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with World Models | [[âœ“](https://arxiv.org/abs/2507.23325)] | âœ— | âœ— | âœ“ | âœ“ | âœ“ | 2.0 |
| **AdaWM** | CVPR25 | AdaWM: Adaptive World Model based Planning for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2501.13072)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **Raw2Drive** | arXiv25 | Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving | [[âœ“](https://arxiv.org/abs/2505.16394)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **VL-SAFE** | arXiv25 | VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2505.16377)] | [[âœ“](https://ys-qu.github.io/vlsafe-website/)] | [[âœ“](https://github.com/ys-qu/vl-safe/tree/main)] | âœ“ | âœ“ | âœ— | 2.0 |
| **LSD-3D** | arXiv25 | LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding | [[âœ“](https://arxiv.org/abs/2508.19204)] | [[âœ“](https://princeton-computational-imaging.github.io/LSD-3D/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **FUTURIST** | arXiv25 | FUTURIST: Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers | [[âœ“](https://arxiv.org/abs/2501.08303)] | âœ— | [[âœ“](https://github.com/Sta8is/FUTURIST)] | âœ“ | âœ— | âœ— | 2.0 |
| **AD-L-JEPA** | arXiv25 | Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data | [[âœ“](https://arxiv.org/abs/2501.04969)] | âœ— | [[âœ“](https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release)] | âœ“ | âœ— | âœ“ | 2.0 |


# ğŸ“„ 2. Progressive Robustness Analysis: 1.0, 2.0 and 3.0

comming soon...

## 2.1
