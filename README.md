# üî• Awesome Robust Driving World Models 

This repository focuses on **driving world models (DWM)** with an emphasis on their **task taxonomy** and **progressive robustness (Robustness 1.0 ‚Üí 3.0)**, base on the survey:

[[**Progressive Robustness-Aware World Models in Autonomous Driving: A Review and Outlook**](https://doi.org/10.36227/techrxiv.176523308.84756413/v1)]

## Citation

If you find this repository or the survey useful, please consider ‚≠ê this repo and citing the paper.

```bibtex
@article{jia2025progressive,
  title   = {{Progressive Robustness-Aware World Models in Autonomous Driving: A Review and Outlook}},
  author  = {Feiyang Jia and Caiyan Jia and Ziying Song and Zhicheng Bao and Lin Liu and Shaoqing Xu and Yan Gong and Lei Yang and Xinyu Zhang and Bin Sun and Xiaoshuai Hao and Long Chen and Yadan Luo},
  journal = {TechRxiv},
  year    = {2025},
  note    = {preprint},
  doi     = {10.36227/techrxiv.176523308.84756413/v1}
}
```

## Paper Recommendations 

If you‚Äôd like to suggest something, please open an new ISSUE page and (if possible) provide:

1. **Online link** to the paper / project homepage / code repository. 
2. The suggested **category**, following this repo:
   - Task: Generation / Planning / Enhancement
   - Robustness level: Robustness 1.0 / 2.0 / 3.0

---

# üìå Overview of Contents

- üìÑ 1. Summary: Information, Task, and Robustness Level
- üõ°Ô∏è 2. Progressive Robustness Analysis: 1.0, 2.0 and 3.0
  - 2.1 Robustness 1.0 ‚Äì Self-Metrics & Evaluation Protocols
  - 2.2 Robustness 2.0 ‚Äì Contributions to Autonomous Driving Systems
  - 2.3 Robustness 3.0 ‚Äì Open-World Robustness & Future Directions

---

# üìÑ 1. Summary: Information, Task, and Robustness Level
**2022**
| Abbr.      | Pub.     | Full Title | Paper  | Page | Code | Gene. | Plan. | Enh. | Lv. |
|:-----------------:|:---------:|:-----------|:------:|:----:|:----:|:----------:|:--------:|:-----------:|:-----------------:|
| **Iso-Dream** | NIPS2022 | Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models | [[‚úì](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9316769afaaeeaad42a9e3633b14e801-Abstract-Conference.html)] | ‚úó | [[‚úì](https://github.com/panmt/Iso-Dream)] | ‚úì | ‚úì | ‚úó | 2.0 |
| **SEM2** | NIPS2022 | Model-based imitation learning for urban driving | [[‚úì](https://arxiv.org/abs/2210.04017)] | ‚úó | ‚úó | ‚úì | ‚úì | ‚úó | 2.0 |
| **MILE** | NIPS2022 | SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model | [[‚úì](https://proceedings.neurips.cc/paper_files/paper/2022/hash/827cb489449ea216e4a257c47e407d18-Abstract-Conference.html)] | ‚úó | [[‚úì](https://github.com/wayveai/mile)] | ‚úì | ‚úì | ‚úó | 2.0 |

**2023**
| Abbr.      | Pub.     | Full Title | Paper  | Page | Code | Gene. | Plan. | Enh. | Lv. |
|:-----------------:|:---------:|:-----------|:------:|:----:|:----:|:----------:|:--------:|:-----------:|:-----------------:|
| **ADriver-I** | arXiv2311 | ADriver-I: A General World Model for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2311.13549)] | ‚úó | ‚úó | ‚úì | ‚úì | ‚úó | 2.0 |
| **GAIA-1** | arXiv2309 | GAIA-1: A Generative World Model for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2309.17080)] | ‚úó | ‚úó | ‚úì | ‚úó | ‚úó | 2.0 |
| **UniWorld** | arXiv2308 | UniWorld: Autonomous Driving Pre-training via World Models | [[‚úì](https://arxiv.org/abs/2308.07234)] | ‚úó | [[‚úì](https://github.com/chaytonmin/UniWorld)] | ‚úì | ‚úó | ‚úì | 2.0 |
| **TrafficBots** | ICRA23 | TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction | [[‚úì](https://arxiv.org/abs/2303.04116)] | ‚úó | [[‚úì](https://github.com/SysCV/TrafficBots)] | ‚úì | ‚úì | ‚úó | 2.0 |

**2024**
| Abbr.      | Pub.     | Full Title | Paper  | Page | Code | Gene. | Plan. | Enh. | Lv. |
|:-----------------:|:---------:|:-----------|:------:|:----:|:----:|:----------:|:--------:|:-----------:|:-----------------:|
|   **DrivingWorld**   | arXiv 2024.12 | DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT                                         | [[‚úì](https://arxiv.org/abs/2412.19505)] |                                      ‚úó                                      |                          [[‚úì](https://github.com/YvanYin/DrivingWorld)]                          |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|  **InfinityDrive**  | arXiv 2024.12 | InfinityDrive: Breaking Time Limits in Driving World Models                                                         | [[‚úì](https://arxiv.org/abs/2412.01522)] | [[‚úì](https://metadrivescape.github.io/papers_project/InfinityDrive/page.html)] |                                               ‚úó                                               |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|      **GenAD**      |   CVPR 2024   | Generalized Predictive Model for Autonomous Driving                                                                 | [[‚úì](https://arxiv.org/abs/2403.09630)] |                                      ‚úó                                      | [[Data](https://github.com/OpenDriveLab/DriveAGI?tab=readme-ov-file#genad-dataset-opendv-youtube)] |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|      **TERRA**      | arXiv 2024.12 | Towards Action Controllable World Models for Autonomous Driving                                                     | [[‚úì](https://arxiv.org/abs/2412.05337)] |                [[‚úì](https://turingmotors.github.io/actbench/)]                |                         [[‚úì](https://github.com/turingmotors/ACT-Bench)]                         |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|      **Vista**      | NeurIPS 2024 | Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability                         | [[‚úì](https://arxiv.org/abs/2405.17398)] |                                      ‚úó                                      |                           [[‚úì](https://github.com/OpenDriveLab/Vista)]                           |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|  **DINO-Foresight**  | arXiv 2024.12 | DINO-Foresight: Self-Supervised Semantic Foresight for Autonomous Driving                                           | [[‚úì](https://arxiv.org/abs/2412.11673)] |                                      ‚úó                                      |                          [[‚úì](https://github.com/Sta8is/DINO-Foresight)]                          |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|   **DriveGenVLM**   |  IAVVC 2024  | DriveGenVLM: Real-world Video Generation for Autonomous Driving with Vision Language Models                         | [[‚úì](https://arxiv.org/abs/2408.16647)] |                                      ‚úó                                      |                                               ‚úó                                               |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|      **Doe-1**      | arXiv 2024.12 | Doe-1: Driving on Earth with One Transformer                                                                        | [[‚úì](https://arxiv.org/abs/2412.09627)] |                                      ‚úó                                      |                               [[‚úì](https://github.com/wzzheng/Doe)]                               |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|     **UniMLVG**     | arXiv 2024.12 | UniMLVG: Unified Multi-View LiDAR-Video Generation for Autonomous Driving                                           | [[‚úì](https://arxiv.org/abs/2412.09628)] |                [[‚úì](https://sensetime-fvg.github.io/UniMLVG/)]                |                          [[‚úì](https://github.com/SenseTime-FVG/OpenDWM)]                          |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|     **Drive-WM**     |   CVPR 2024   | Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving          | [[‚úì](https://arxiv.org/abs/2311.17918)] |                       [[‚úì](https://drive-wm.github.io/)]                       |                           [[‚úì](https://github.com/BraveGroup/Drive-WM)]                           |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|   **DriveDreamer**   |   ECCV 2024   | DriveDreamer: Towards Real-world-driven Generative World Models for Autonomous Driving                              | [[‚úì](https://arxiv.org/abs/2309.09777)] |                     [[‚úì](https://drivedreamer.github.io/)]                     |                        [[‚úì](https://github.com/JeffWang987/DriveDreamer)]                        |  ‚úì  |  ‚úì  |  ‚úì  | 2.0 |
| **DrivingDiffusion** |   ECCV 2024   | Layout-Guided multi-view driving scene video generation with latent diffusion model                                 | [[‚úì](https://arxiv.org/abs/2310.07771)] |                   [[‚úì](https://drivingdiffusion.github.io/)]                   |                        [[‚úì](https://github.com/shalfun/DrivingDiffusion)]                        |  ‚úì  |  ‚úó  |  ‚úì  | 2.0 |
|   **DrivePhysica**   | arXiv 2024.12 | DrivePhysica: Physical-Consistent Video Generation for Autonomous Driving                                           | [[‚úì](https://arxiv.org/abs/2412.09621)] |                                      ‚úó                                      |           [[‚úì](https://metadrivescape.github.io/papers_project/DrivePhysica/page.html)]           |  ‚úì  |  ‚úó  |  ‚úì  | 2.0 |
|     **Panacea**     |   CVPR 2024   | Panoramic and Controllable Video Generation for Autonomous Driving                                                  | [[‚úì](https://arxiv.org/abs/2311.16813)] |                      [[‚úì](https://panacea-ad.github.io/)]                      |                            [[‚úì](https://github.com/wenyuqing/panacea)]                            |  ‚úì  |  ‚úó  |  ‚úì  | 2.0 |
|    **DriveScape**    | arXiv 2024.09 | DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation                                | [[‚úì](https://arxiv.org/abs/2409.05463)] |                                      ‚úó                                      |                                               ‚úó                                               |  ‚úì  |  ‚úó  |  ‚úì  | 2.0 |
|    **HoloDrive**    | arXiv 2024.12 | Holistic 2D-3D Multi-Modal Street Scene Generation for Autonomous Driving                                                      | [[‚úì](https://arxiv.org/abs/2412.01407)] |                                      ‚úó                                      |                                               ‚úó                                               |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|     **WoVoGen**     |   ECCV 2024   | World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation                                                   | [[‚úì](https://arxiv.org/abs/2312.02934)] |                      ‚úó                       |             [[‚úì](https://github.com/fudan-zvg/WoVoGen)]                                                                                  |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|    **Copilot4D**    |   ICLR 2024   | Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion                                                  | [[‚úì](https://arxiv.org/abs/2311.01017)] |                                      ‚úó                                      |                     ‚úó                          |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|  **DFIT-OccWorld**  | arXiv 2024.12 | An Efficient Occupancy World Model via Decoupled Dynamic Flow and Image-assisted Training                                                             | [[‚úì](https://arxiv.org/abs/2412.13772)] |                                      ‚úó                                      |                                               ‚úó                                               |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|      **ViDAR**      |   CVPR 2024   | Visual Point Cloud Forecasting enables Scalable Autonomous Driving                                                                  | [[‚úì](https://arxiv.org/abs/2312.17655)] |                                      ‚úó                                      |                           [[‚úì](https://github.com/OpenDriveLab/ViDAR)]                           |  ‚úì  |  ‚úì  |  ‚úì  | 2.0 |
|       **UnO**       |   CVPR 2024   | Unsupervised Occupancy Fields for Perception and Forecasting                                                  | [[‚úì](https://arxiv.org/abs/2406.08691)] |                                       [[‚úì](https://waabi.ai/research/uno)]                                      |                             [[‚úì](https://waabi.ai/research/uno)]                              |  ‚úì  |  ‚úó  |  ‚úì  | 2.0 |
|     **OccWorld**     |   ECCV 2024   | OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving                                                | [[‚úì](https://arxiv.org/abs/2311.16038)] |                      [[‚úì](https://wzzheng.net/OccWorld)]                      |                            [[‚úì](https://github.com/wzzheng/OccWorld)]                            |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|       **DOME**       | arXiv 2024.10 | Taming Diffusion Model into High-Fidelity Controllable Occupancy World Model                                              | [[‚úì](https://arxiv.org/abs/2410.10429)] |                     [[‚úì](https://gusongen.github.io/DOME)]                     |                 [[‚úì](https://github.com/gusongen/DOME)]                                                                                |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|    **DriveWorld**    |   CVPR 2024   | 4D Pre-trained Scene Understanding via World Models for Autonomous Driving                                                                  | [[‚úì](https://arxiv.org/abs/2405.04390)] |                                      ‚úó                                      |                       ‚úó                            |  ‚úì  |  ‚úì  |  ‚úì  | 2.0 |
|     **Cam4DOCC**     |   CVPR 2024   | Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications                     | [[‚úì](https://arxiv.org/abs/2311.17663)] |                            ‚úó            |                            [[‚úì](https://github.com/haomo-ai/Cam4DOcc)]                            |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|     **OccSora**     | arXiv 2024.05 | OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving                                  | [[‚úì](https://arxiv.org/abs/2405.17833)] |                                      ‚úó                                      |                             [[‚úì](https://github.com/haomo-ai/Cam4DOcc)]                             |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|       **NeMo**       |   ECCV 2024   | Neural Volumetric World Models for Autonomous Driving                                                                                 | [[‚úì](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02571.pdf)] |                                      ‚úó                                      |                            ‚úó                            |  ‚úì  |  ‚úì  |  ‚úì  | 2.0 |
|     **OccLLaMA**     | arXiv 2024.09 | An Occupancy-Language-Action Generative World Model for Autonomous Driving                                                       | [[‚úì](https://arxiv.org/abs/2409.03272)] |                                      ‚úó                                      |                            ‚úó                            |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|       **LAW**       | arXiv 2024.06 | Enhancing End-to-end Autonomous Driving with Latent World Model                                                     | [[‚úì](https://arxiv.org/abs/2406.08481)] |                                      ‚úó                                      |                             [[‚úì](https://github.com/BraveGroup/LAW)]                             |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|    **CarFormer**    |   ECCV 2024   | CarFormer: Self-Driving with Learned Object-Centric Representations                                                 | [[‚úì](https://arxiv.org/abs/2407.15843)] |                  [[‚úì](https://kuis-ai.github.io/CarFormer/)]                  |                            [[‚úì](https://github.com/Shamdan17/CarFormer)]                            |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|      **GenAD**      |   ECCV 2024   | Generative End-to-End Autonomous Driving                                                                     | [[‚úì](https://arxiv.org/abs/2402.11502)] |                     ‚úó                      |                              [[‚úì](https://github.com/wzzheng/GenAD)]                              |  ‚úì  |  ‚úì  |  ‚úì  | 2.0 |
|  **SceneDiffuser**  | NeurIPS 2024 | Efficient and Controllable Driving Simulation Initialization and Rollout                                                   | [[‚úì](https://arxiv.org/abs/2412.12129)] |                                      ‚úó                                      |                         ‚úó                     |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|     **MARL-CCE**     |   ECCV 2024   | Modelling Competitive Behaviors in Autonomous Driving Under Generative World Model      | [[‚úì](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05085.pdf)] |                                      ‚úó                                      |                          [[‚úì](https://github.com/qiaoguanren/MARL-CCE)]                          |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|      **RAMBLE**      | arXiv 2024.10 | From Imitation to Exploration: End-to-end Autonomous Driving based on World Model                                                               | [[‚úì](https://arxiv.org/abs/2410.02253)] |           ‚úó           |                              ‚úó                              |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
| **Imagine-2-Drive** | arXiv 2024.11 | High-Fidelity World Modeling in CARLA for Autonomous Vehicles                                                       | [[‚úì](https://arxiv.org/abs/2411.10171)] |                                   [[‚úì](https://anantagrg.github.io/Imagine-2-Drive.github.io/)]                                         |                                               ‚úó                                               |  ‚úì  |  ‚úó  |  ‚úó  | 2.0 |
|   **Popov et al.**   | arXiv 2024.09 | Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models | [[‚úì](https://arxiv.org/abs/2409.16663)] |                                      ‚úó                                      |                                               ‚úó                                               |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|   **Think2Drive**   |   ECCV 2024   | Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving      | [[‚úì](https://arxiv.org/abs/2402.16720)] |                 ‚úó                 |                          ‚úó                        |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |
|       **GUMP**       |   ECCV 2024   | Solving Motion Planning Tasks with a Scalable Generative Model                                                                    | [[‚úì](https://arxiv.org/abs/2407.02797)] |                  ‚úó                  |                               [[‚úì](https://github.com/HorizonRobotics/GUMP/)]                               |  ‚úì  |  ‚úì  |  ‚úó  | 2.0 |

**2025**
| Abbr.      | Pub.     | Full Title | Paper  | Page | Code | Gene. | Plan. | Enh. | Lv. |
|:-----------------:|:---------:|:-----------|:------:|:----:|:----:|:----------:|:--------:|:-----------:|:-----------------:|
| **HERMES** | A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation | [[‚úì](https://arxiv.org/abs/2505.16394)] | [[‚úì](https://lmd0311.github.io/HERMES/)] | [[‚úì](https://github.com/LMD0311/HERMES)] |
| **DINO-Foresight** | Looking into the Future with DINO | [[‚úì](https://arxiv.org/abs/2412.11673)] | ‚úó | [[‚úì](https://github.com/Sta8is/DINO-Foresight)] |
| **From Forecasting to Planning** | Policy World Model for Collaborative State-Action Prediction | [[‚úì](https://arxiv.org/abs/2510.19654)] | ‚úó | [[‚úì](https://github.com/6550Zhao/Policy-World-Model)] |
| **InfiniCube** | Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models | [[‚úì](https://arxiv.org/abs/2412.03934)] | [[‚úì](https://research.nvidia.com/labs/toronto-ai/infinicube/)] | ‚úó |
| **DiST-4D** | Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation | [[‚úì](https://arxiv.org/abs/2503.15208)] | [[‚úì](https://royalmelon0505.github.io/DiST-4D/)] | ‚úó |
| **Epona** | Autoregressive Diffusion World Model for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2506.24113)] | ‚úó | [[‚úì](https://github.com/Kevin-thu/Epona/)] |
| **UniOcc** | A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving | [[‚úì](https://arxiv.org/abs/2503.24381)] | [[‚úì](https://uniocc.github.io/)] | ‚úó |
| **DriVerse** | Navigation World Model for Driving Simulation via Multimodal Trajectory Prompting and Motion Alignment | [[‚úì](https://arxiv.org/abs/2504.19614)] | ‚úó | [[‚úì](https://github.com/shalfun/DriVerse)] |
| **World4Drive** | End-to-End Autonomous Driving via Intention-aware Physical Latent World Model | [[‚úì](https://arxiv.org/abs/2507.00603)] | ‚úó | ‚úó |
| **PIWM** | Dream to Drive with Predictive Individual World Model | [[‚úì](https://arxiv.org/abs/2501.16733)] | ‚úó | [[‚úì](https://github.com/gaoyinfeng/PIWM)] |
| **DriveDreamer4D** | World Models Are Effective Data Machines for 4D Driving Scene Representation | [[‚úì](https://arxiv.org/abs/2410.13571)] | [[‚úì](https://drivedreamer4d.github.io/)] | ‚úó |
| **GaussianWorld** | Gaussian World Model for Streaming 3D Occupancy Prediction | [[‚úì](https://arxiv.org/abs/2412.10373)] | ‚úó | [[‚úì](https://github.com/zuosc19/GaussianWorld)] |
| **ReconDreamer** | Crafting World Models for Driving Scene Reconstruction via Online Restoration | [[‚úì](https://arxiv.org/abs/2411.19548)] | ‚úó | [[‚úì](https://github.com/GigaAI-research/ReconDreamer)] |
| **FUTURIST** | Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers | [[‚úì](https://arxiv.org/abs/2501.08303)] | ‚úó | [[‚úì](https://github.com/Sta8is/FUTURIST)] |
| **MaskGWM** | A Generalizable Driving World Model with Video Mask Reconstruction | [[‚úì](https://arxiv.org/abs/2502.11663)] | ‚úó | [[‚úì](https://github.com/SenseTime-FVG/OpenDWM)] |
| **UniScene** | Unified Occupancy-centric Driving Scene Generation | [[‚úì](https://arxiv.org/abs/2412.05435)] | [[‚úì](https://arlo0o.github.io/uniscene/)] | ‚úó |
| **GEM** | A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control | [[‚úì](https://arxiv.org/abs/2412.11198)] | [[‚úì](https://vita-epfl.github.io/GEM.github.io/)] | ‚úó |
| **UMGen** | Generating Multimodal Driving Scenes via Next-Scene Prediction | [[‚úì](https://arxiv.org/abs/2503.14945)] | [[‚úì](https://yanhaowu.github.io/UMGen/)] | [[‚úì](https://github.com/YanhaoWu/UMGen/)] |
| **DIO** | Decomposable Implicit 4D Occupancy-Flow World Model | [[‚úì](https://openaccess.thecvf.com/content/CVPR2025/html/Diehl_DIO_Decomposable_Implicit_4D_Occupancy-Flow_World_Model_CVPR_2025_paper.html)] | ‚úó | ‚úó |
| **SceneDiffuser++** | City-Scale Traffic Simulation via a Generative World Model | [[‚úì](https://openaccess.thecvf.com/content/CVPR2025/html/Tan_SceneDiffuser_City-Scale_Traffic_Simulation_via_a_Generative_World_Model_CVPR_2025_paper.html)] | ‚úó | ‚úó |
| **DynamicCity** | Large-Scale LiDAR Generation from Dynamic Scenes | [[‚úì](https://arxiv.org/abs/2410.18084)] | ‚úó | [[‚úì](https://github.com/3DTopia/DynamicCity)] |
| **AdaWM** | Adaptive World Model based Planning for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2501.13072)] | ‚úó | ‚úó |
| **OccProphet** | Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework | [[‚úì](https://arxiv.org/abs/2502.15180)] | ‚úó | [[‚úì](https://github.com/JLChen-C/OccProphet)] |
| **PreWorld** | Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2502.07309)] | ‚úó | [[‚úì](https://github.com/getterupper/PreWorld)] |
| **SSR** | Does End-to-End Autonomous Driving Really Need Perception Tasks? | [[‚úì](https://arxiv.org/abs/2409.18341)] | ‚úó | [[‚úì](https://github.com/PeidongLi/SSR)] |
| **Occ-LLM** | Enhancing Autonomous Driving with Occupancy-Based Large Language Models | [[‚úì](https://arxiv.org/abs/2502.06419)] | ‚úó | ‚úó |
| **STAGE** | A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation | [[‚úì](https://arxiv.org/abs/2506.13138)] | [[‚úì](https://4dvlab.github.io/STAGE/)] | ‚úó |
| **Drive&Gen** | Co-Evaluating End-to-End Driving and Video Generation Models | [[‚úì](https://arxiv.org/abs/2510.06209)] | ‚úó | ‚úó |
| **Learning to Generate 4D LiDAR Sequences** | Learning to Generate 4D LiDAR Sequences | [[‚úì](https://arxiv.org/abs/2509.11959)] | ‚úó | ‚úó |
| **Accident Anticipation WM** | World model-based end-to-end scene generation for accident anticipation in autonomous driving | [[‚úì](https://www.nature.com/articles/s44172-025-00474-7)] | ‚úó | ‚úó |
| **LIDAR Navigation WM** | World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations | [[‚úì](https://arxiv.org/abs/2512.03429)] | ‚úó | ‚úó |
| **UniFuture** | Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception | [[‚úì](https://arxiv.org/abs/2503.13587)] | [[‚úì](https://dk-liang.github.io/UniFuture/)] | [[‚úì](https://github.com/dk-liang/UniFuture)] |
| **MindDrive** | An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving | [[‚úì](https://arxiv.org/abs/2512.04441)] | ‚úó | ‚úó |
| **U4D** | Uncertainty-Aware 4D World Modeling from LiDAR Sequences | [[‚úì](https://arxiv.org/abs/2512.02982)] | ‚úó | ‚úó |
| **Think Before You Drive** | World Model-Inspired Multimodal Grounding for Autonomous Vehicles | [[‚úì](https://arxiv.org/abs/2512.03454)] | ‚úó | ‚úó |
| **Vehicle Dynamics WM** | Vehicle Dynamics Embedded World Models for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2512.02417)] | ‚úó | ‚úó |
| **LiSTAR** | Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving | [[‚úì](https://arxiv.org/abs/2511.16049)] | [[‚úì](https://ocean-luna.github.io/LiSTAR.gitub.io)] | ‚úó |
| **OpenTwinMap** | An Open-Source Digital Twin Generator for Urban Autonomous Driving | [[‚úì](https://arxiv.org/abs/2511.21925)] | ‚úó | ‚úó |
| **SparseWorld-TC** | Trajectory-Conditioned Sparse Occupancy World Model | [[‚úì](https://arxiv.org/abs/2511.22039)] | ‚úó | ‚úó |
| **LaGen** | Towards Autoregressive LiDAR Scene Generation | [[‚úì](https://arxiv.org/abs/2511.21256)] | ‚úó | ‚úó |
| **AD-R1** | Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models | [[‚úì](https://arxiv.org/abs/2511.20325)] | ‚úó | ‚úó |
| **CorrectAD** | A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving | [[‚úì](https://arxiv.org/abs/2511.13297)] | ‚úó | ‚úó |
| **UniScenev2** | Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method | [[‚úì](https://arxiv.org/abs/2510.22973)] | ‚úó | ‚úó |
| **Vision-Centric 4D Occ** | Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models | [[‚úì](https://arxiv.org/abs/2510.16729)] | ‚úó | ‚úó |
| **SparseWorld** | A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries | [[‚úì](https://arxiv.org/abs/2510.17482)] | ‚úó | [[‚úì](https://github.com/MSunDYY/SparseWorld)] |
| **OmniNWM** | Omniscient Driving Navigation World Models | [[‚úì](https://arxiv.org/abs/2510.18313)] | [[‚úì](https://arlo0o.github.io/OmniNWM/)] | ‚úó |
| **ORAD-3D** | Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks | [[‚úì](https://arxiv.org/abs/2510.16500)] | ‚úó | [[‚úì](https://github.com/chaytonmin/ORAD-3D)] |
| **Dream4Drive** | Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks | [[‚úì](https://arxiv.org/abs/2510.19195)] | [[‚úì](https://wm-research.github.io/Dream4Drive/)] | ‚úó |
| **DriveVLA-W0** | World Models Amplify Data Scaling Law in Autonomous Driving | [[‚úì](https://arxiv.org/abs/2510.12796)] | ‚úó | ‚úó |
| **CoIRL-AD** | Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2510.12560)] | ‚úó | ‚úó |
| **CVD-STORM** | Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2510.07944)] | ‚úó | ‚úó |
| **PhiGensis** | 4D Driving Scene Generation With Stereo Forcing | [[‚úì](https://arxiv.org/abs/2509.20251)] | [[‚úì](https://jiangxb98.github.io/PhiGensis/)] | ‚úó |
| **TeraSim-World** | Worldwide Safety-Critical Data Synthesis for End-to-End Autonomous Driving | [[‚úì](https://arxiv.org/abs/2509.13164)] | ‚úó | ‚úó |
| **OccTENS** | 3D Occupancy World Model via Temporal Next-Scale Prediction | [[‚úì](https://arxiv.org/abs/2509.03887)] | ‚úó | ‚úó |
| **G^2Editor** | Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation | [[‚úì](https://arxiv.org/abs/2508.20471)] | ‚úó | ‚úó |
| **LSD-3D** | Large-Scale 3D Driving Scene Generation with Geometry Grounding | [[‚úì](https://arxiv.org/abs/2508.19204)] | [[‚úì](https://princeton-computational-imaging.github.io/LSD-3D/)] | ‚úó |
| **Fine-Tuned Video Gen** | Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation | [[‚úì](https://arxiv.org/abs/2508.16512)] | ‚úó | ‚úó |
| **MoVieDrive** | Multi-Modal Multi-View Urban Scene Video Generation | [[‚úì](https://arxiv.org/abs/2508.14327)] | ‚úó | ‚úó |
| **ImagiDrive** | A Unified Imagination-and-Planning Framework for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2508.11428)] | ‚úó | [[‚úì](https://github.com/fudan-zvg/ImagiDrive)] |
| **LiDARCrafter** | Dynamic 4D World Modeling from LiDAR Sequences | [[‚úì](https://arxiv.org/abs/2508.03692)] | [[‚úì](https://lidarcrafter.github.io/)] | ‚úó |
| **FASTopoWM** | Fast-Slow Lane Segment Topology Reasoning with Latent World Models | [[‚úì](https://arxiv.org/abs/2507.23325)] | ‚úó | ‚úó |
| **Accident Anticipation II** | World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving | [[‚úì](https://arxiv.org/abs/2507.12762)] | ‚úó | ‚úó |
| **Orbis** | Overcoming Challenges of Long-Horizon Prediction in Driving World Models | [[‚úì](https://arxiv.org/abs/2507.13162)] | [[‚úì](https://lmb-freiburg.github.io/orbis.github.io/)] | ‚úó |
| **I2-World** | Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting | [[‚úì](https://arxiv.org/abs/2507.09144)] | ‚úó | [[‚úì](https://github.com/lzzzzzm/II-World)] |
| **NRSeg** | Noise-Resilient Learning for BEV Semantic Segmentation via Driving World Models | [[‚úì](https://arxiv.org/abs/2507.04002)] | ‚úó | [[‚úì](https://github.com/lynn-yu/NRSeg)] |
| **LiDAR Foundational** | Towards foundational LiDAR world models with efficient latent flow matching | [[‚úì](https://arxiv.org/abs/2506.23434)] | ‚úó | ‚úó |
| **ReSim** | Reliable World Simulation for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2506.09981)] | [[‚úì](https://opendrivelab.com/ReSim)] | ‚úó |
| **Cosmos-Drive-Dreams** | Scalable Synthetic Driving Data Generation with World Foundation Models | [[‚úì](https://arxiv.org/abs/2506.09042)] | [[‚úì](https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams/)] | ‚úó |
| **Dreamland** | Controllable World Creation with Simulator and Generative Models | [[‚úì](https://arxiv.org/abs/2506.08006)] | [[‚úì](https://metadriverse.github.io/dreamland/)] | ‚úó |
| **LongDWM** | Cross-Granularity Distillation for Building a Long-Term Driving World Model | [[‚úì](https://arxiv.org/abs/2506.01546)] | [[‚úì](https://wang-xiaodong1899.github.io/longdwm/)] | ‚úó |
| **FutureSightDrive** | Thinking Visually with Spatio-Temporal CoT for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2505.17685)] | ‚úó | [[‚úì](https://github.com/MIV-XJTU/FSDrive)] |
| **ProphetDWM** | ProphetDWM: A Driving World Model for Rolling Out Future Actions and Videos | [[‚úì](https://arxiv.org/abs/2505.18650)] | ‚úó | ‚úó |
| **GeoDrive** | 3D Geometry-Informed Driving World Model with Precise Action Control | [[‚úì](https://arxiv.org/abs/2505.22421)] | ‚úó | [[‚úì](https://github.com/antonioo-c/GeoDrive)] |
| **DriveX** | Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving | [[‚úì](https://arxiv.org/abs/2505.19239)] | ‚úó | ‚úó |
| **VL-SAFE** | Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2505.16377)] | [[‚úì](https://ys-qu.github.io/vlsafe-website/)] | ‚úó |
| **Raw2Drive** | Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2) | [[‚úì](https://arxiv.org/abs/2505.16394)] | ‚úó | ‚úó |
| **RAMBLE** | From Imitation to Exploration: End-to-end Autonomous Driving based on World Model | [[‚úì](https://arxiv.org/abs/2410.02253)] | ‚úó | [[‚úì](https://github.com/SCP-CN-001/ramble)] |
| **DiVE** | Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer | [[‚úì](https://arxiv.org/abs/2504.18576)] | ‚úó | ‚úó |
| **WoTE** | End-to-End Driving with Online Trajectory Evaluation via BEV World Model | [[‚úì](https://arxiv.org/abs/2503.22231)] | ‚úó | [[‚úì](https://github.com/liyingyanUCAS/WoTE)] |
| **MagicDrive-V2** | High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control | [[‚úì](https://arxiv.org/abs/2411.13807)] | [[‚úì](https://gaoruiyuan.com/magicdrive-v2/)] | ‚úó |
| **CoGen** | 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2503.22231)] | ‚úó | ‚úó |
| **GAIA-2** | A Controllable Multi-View Generative World Model for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2503.20523)] | ‚úó | ‚úó |
| **Semi-SD** | Semi-Supervised Metric Depth Estimation via Surrounding Cameras for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2503.19713)] | ‚úó | [[‚úì](https://github.com/xieyuser/Semi-SD)] |
| **MiLA** | Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving | [[‚úì](https://arxiv.org/abs/2503.15875)] | [[‚úì](https://xiaomi-mlab.github.io/mila.github.io/)] | ‚úó |
| **SimWorld** | A Unified Benchmark for Simulator-Conditioned Scene Generation via World Model | [[‚úì](https://arxiv.org/abs/2503.13952)] | ‚úó | [[‚úì](https://github.com/Li-Zn-H/SimWorld)] |
| **EOT-WM** | Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latant Space | [[‚úì](https://arxiv.org/abs/2503.09215)] | ‚úó | ‚úó |
| **T¬≥Former** | Temporal Triplane Transformers as Occupancy World Models | [[‚úì](https://arxiv.org/abs/2503.07338)] | ‚úó | ‚úó |
| **AVD2** | Accident Video Diffusion for Accident Video Description | [[‚úì](https://arxiv.org/abs/2502.14801)] | [[‚úì](https://an-answer-tree.github.io/)] | ‚úó |
| **VaViM and VaVAM** | Autonomous Driving through Video Generative Modeling | [[‚úì](https://arxiv.org/abs/2502.15672)] | ‚úó | [[‚úì](https://github.com/valeoai/VideoActionModel)] |
| **Dream to Drive** | Model-Based Vehicle Control Using Analytic World Models | [[‚úì](https://arxiv.org/abs/2502.10012)] | ‚úó | ‚úó |
| **AD-L-JEPA** | Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data | [[‚úì](https://arxiv.org/abs/2501.04969)] | ‚úó | [[‚úì](https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release)] |

# üìÑ 2. Progressive Robustness Analysis: 1.0, 2.0 and 3.0

comming soon...

## 2.1
