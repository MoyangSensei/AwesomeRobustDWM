# ğŸ”¥ Awesome Robust Driving World Models 

This repository focuses on **driving world models (DWM)** with an emphasis on their **task taxonomy** and **progressive robustness (Robustness 1.0 â†’ 3.0)**, base on the survey:

[[**Progressive Robustness-Aware World Models in Autonomous Driving: A Review and Outlook**](https://doi.org/10.36227/techrxiv.176523308.84756413/v1)]

## Citation

If you find this repository or the survey useful, please consider â­ this repo and citing the paper.

```bibtex
@article{jia2025progressive,
  title   = {{Progressive Robustness-Aware World Models in Autonomous Driving: A Review and Outlook}},
  author  = {Feiyang Jia and Caiyan Jia and Ziying Song and Zhicheng Bao and Lin Liu and Shaoqing Xu and Yan Gong and Lei Yang and Xinyu Zhang and Bin Sun and Xiaoshuai Hao and Long Chen and Yadan Luo},
  journal = {TechRxiv},
  year    = {2025},
  note    = {preprint},
  doi     = {10.36227/techrxiv.176523308.84756413/v1}
}
```

## Paper Recommendations 

If youâ€™d like to suggest something, please open an new ISSUE page and (if possible) provide:

1. **Online link** to the paper / project homepage / code repository. 
2. The suggested **category**, following this repo:
   - Task: Generation / Planning / Enhancement
   - Robustness level: Robustness 1.0 / 2.0 / 3.0

---

# ğŸ“Œ Overview of Contents

- ğŸ“„ 1. Summary: Information, Task, and Robustness Level
- ğŸ›¡ï¸ 2. Progressive Robustness Analysis: 1.0, 2.0 and 3.0
  - 2.1 Robustness 1.0 â€“ Self-Metrics & Evaluation Protocols
  - 2.2 Robustness 2.0 â€“ Contributions to Autonomous Driving Systems
  - 2.3 Robustness 3.0 â€“ Open-World Robustness & Future Directions

---

# ğŸ“„ 1. Summary: Information, Task, and Robustness Level
**2022**
| Abbr.      | Pub.     | Full Title | Paper  | Page | Code | Gene. | Plan. | Enh. | Lv. |
|:-----------------:|:---------:|:-----------|:------:|:----:|:----:|:----------:|:--------:|:-----------:|:-----------------:|
| **Iso-Dream** | NIPS2022 | Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models | [[âœ“](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9316769afaaeeaad42a9e3633b14e801-Abstract-Conference.html)] | âœ— | [[âœ“](https://github.com/panmt/Iso-Dream)] | âœ“ | âœ“ | âœ— | 2.0 |
| **SEM2** | NIPS2022 | Model-based imitation learning for urban driving | [[âœ“](https://arxiv.org/abs/2210.04017)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **MILE** | NIPS2022 | SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model | [[âœ“](https://proceedings.neurips.cc/paper_files/paper/2022/hash/827cb489449ea216e4a257c47e407d18-Abstract-Conference.html)] | âœ— | [[âœ“](https://github.com/wayveai/mile)] | âœ“ | âœ“ | âœ— | 2.0 |

**2023**
| Abbr.      | Pub.     | Full Title | Paper  | Page | Code | Gene. | Plan. | Enh. | Lv. |
|:-----------------:|:---------:|:-----------|:------:|:----:|:----:|:----------:|:--------:|:-----------:|:-----------------:|
| **ADriver-I** | arXiv2311 | ADriver-I: A General World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2311.13549)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **GAIA-1** | arXiv2309 | GAIA-1: A Generative World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2309.17080)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **UniWorld** | arXiv2308 | UniWorld: Autonomous Driving Pre-training via World Models | [[âœ“](https://arxiv.org/abs/2308.07234)] | âœ— | [[âœ“](https://github.com/chaytonmin/UniWorld)] | âœ“ | âœ— | âœ“ | 2.0 |
| **TrafficBots** | ICRA23 | TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction | [[âœ“](https://arxiv.org/abs/2303.04116)] | âœ— | [[âœ“](https://github.com/SysCV/TrafficBots)] | âœ“ | âœ“ | âœ— | 2.0 |

**2024**
| Abbr.      | Pub.     | Full Title | Paper  | Page | Code | Gene. | Plan. | Enh. | Lv. |
|:-------:|:------:|:-----------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:--------:|
| **SEM2** | TITS | Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model | [[âœ“](https://ieeexplore.ieee.org/abstract/document/10538211/)] | âœ— | [[âœ“](https://github.com/TRAILab/SEM2)] | âœ“ | âœ“ | âœ— | 2.0 |
| **Vista** | NeurIPS 2024 | Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability | [[âœ“](https://arxiv.org/abs/2405.17398)] | âœ— | [[âœ“](https://github.com/OpenDriveLab/Vista)] | âœ“ | âœ“ | âœ— | 2.0 |
| **SceneDiffuser** | NeurIPS 2024 | SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout | [[âœ“](https://arxiv.org/abs/2412.12129)] | âœ— | [[âœ“](https://github.com/alipay/SceneDiffuser)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DrivingDojo** | NeurIPS 2024 | DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model | [[âœ“](https://arxiv.org/abs/2410.10738)] | [[âœ“](https://drivingdojo.github.io/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Think2Drive** | ECCV 2024 | Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving | [[âœ“](https://arxiv.org/abs/2402.16720)] | âœ— | [[âœ“](https://github.com/trust-ai/Think2Drive)] | âœ“ | âœ“ | âœ— | 2.0 |
| **MARL-CCE** | ECCV 2024 | Modelling Competitive Behaviors in Autonomous Driving Under Generative World Model | [[âœ“](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05085.pdf)] | âœ— | [[âœ“](https://github.com/qiaoguanren/MARL-CCE)] | âœ“ | âœ— | âœ— | 2.0 |
| **DriveDreamer** | ECCV 2024 | DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2309.09777)] | âœ— | [[âœ“](https://github.com/JeffWang987/DriveDreamer)] | âœ“ | âœ“ | âœ“ | 2.0 |
| **GenAD** | ECCV 2024 | GenAD: Generative End-to-End Autonomous Driving | [[âœ“](https://arxiv.org/abs/2402.11502)] | âœ— | [[âœ“](https://github.com/wzzheng/GenAD)] | âœ“ | âœ“ | âœ“ | 2.0 |
| **OccWorld** | ECCV 2024 | OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2311.16038)] | âœ— | [[âœ“](https://github.com/wzzheng/OccWorld)] | âœ“ | âœ“ | âœ— | 2.0 |
| **NeMo** | ECCV 2024 | Neural Volumetric World Models for Autonomous Driving | [[âœ“](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02571.pdf)] | âœ— | [[âœ“](https://github.com/MARS-Lab-CV/NeMo)] | âœ“ | âœ“ | âœ“ | 2.0 |
| **CarFormer** | ECCV 2024 | CarFormer: Self-Driving with Learned Object-Centric Representations | [[âœ“](https://arxiv.org/abs/2407.15843)] | âœ— | [[âœ“](https://kuis-ai.github.io/CarFormer/)] | âœ“ | âœ“ | âœ— | 2.0 |
| **GUMP** | ECCV 2024 | GUMP: Solving Motion Planning Tasks with a Scalable Generative Model | [[âœ“](https://arxiv.org/abs/2407.02797)] | âœ— | [[âœ“](https://github.com/HorizonRobotics/GUMP/)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DrivingDiffusion** | ECCV 2024 | DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model | [[âœ“](https://arxiv.org/abs/2310.07771)] | âœ— | [[âœ“](https://github.com/shalfun/DrivingDiffusion)] | âœ“ | âœ— | âœ“ | 2.0 |
| **3D-VLA** | ICML 2024 | 3D-VLA: A 3D Vision-Language-Action Generative World Model | [[âœ“](https://arxiv.org/abs/2403.09631)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **ViDAR** | CVPR 2024 | ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving | [[âœ“](https://arxiv.org/abs/2312.17655)] | âœ— | [[âœ“](https://github.com/OpenDriveLab/ViDAR)] | âœ“ | âœ“ | âœ“ | 2.0 |
| **GenAD** | CVPR 2024 | Generalized Predictive Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2403.09630)] | âœ— | [[Data](https://github.com/OpenDriveLab/DriveAGI?tab=readme-ov-file#genad-dataset-opendv-youtube)] | âœ“ | âœ“ | âœ— | 2.0 |
| **Cam4DOCC** | CVPR 2024 | Cam4DOCC: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications | [[âœ“](https://arxiv.org/abs/2311.17663)] | âœ— | [[âœ“](https://github.com/haomo-ai/Cam4DOcc)] | âœ“ | âœ— | âœ— | 2.0 |
| **Drive-WM** | CVPR 2024 | Drive-WM: Driving into the Future: Multiview Visual Forecasting and Planning with World Model | [[âœ“](https://arxiv.org/abs/2311.17918)] | âœ— | [[âœ“](https://github.com/BraveGroup/Drive-WM)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DriveWorld** | CVPR 2024 | DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2405.04390)] | âœ— | [[âœ“](https://github.com/OpenDriveLab/DriveWorld)] | âœ“ | âœ“ | âœ“ | 2.0 |
| **Panacea** | CVPR 2024 | Panacea: Panoramic and Controllable Video Generation for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2311.16813)] | âœ— | [[âœ“](https://panacea-ad.github.io/)] | âœ“ | âœ— | âœ“ | 2.0 |
| **UnO** | CVPR 2024 | UnO: Unsupervised Occupancy Fields for Perception and Forecasting | [[âœ“](https://arxiv.org/abs/2406.08691)] | âœ— | [[âœ“](https://waabi.ai/research/uno)] | âœ“ | âœ— | âœ“ | 2.0 |
| **MagicDrive** | ICLR 2024 | MagicDrive: Street View Generation with Diverse 3D Geometry Control | [[âœ“](https://arxiv.org/abs/2310.02601)] | âœ— | [[âœ“](https://github.com/cure-lab/MagicDrive)] | âœ“ | âœ— | âœ“ | 2.0 |
| **Copilot4D** | ICLR 2024 | Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion | [[âœ“](https://arxiv.org/abs/2311.01017)] | âœ— | [[âœ“](https://github.com/edward19971023/Copilot4D)] | âœ“ | âœ— | âœ— | 2.0 |
| **SafeDreamer** | ICLR 2024 | SafeDreamer: Safe Reinforcement Learning with World Models | [[âœ“](https://openreview.net/forum?id=tsE5HLYtYg)] | âœ— | [[âœ“](https://github.com/PKU-Alignment/SafeDreamer)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DrivingWorld** | arXiv 2024.12 | DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT | [[âœ“](https://arxiv.org/abs/2412.19505)] | âœ— | [[âœ“](https://github.com/YvanYin/DrivingWorld)] | âœ“ | âœ— | âœ— | 2.0 |
| **DrivingGPT** | arXiv 2024.12 | DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers | [[âœ“](https://arxiv.org/abs/2412.18607)] | [[âœ“](https://rogerchern.github.io/DrivingGPT/)] | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **DFIT-OccWorld** | arXiv 2024.12 | An Efficient Occupancy World Model via Decoupled Dynamic Flow and Image-assisted Training | [[âœ“](https://arxiv.org/abs/2412.13772)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **Doe-1** | arXiv 2024.12 | Doe-1: Closed-Loop Autonomous Driving with Large World Model | [[âœ“](https://arxiv.org/abs/2412.09627)] | âœ— | [[âœ“](https://github.com/wzzheng/Doe)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DrivePhysica** | arXiv 2024.12 | DrivePhysica: Physical Informed Driving World Model | [[âœ“](https://arxiv.org/abs/2412.08410)] | âœ— | [[âœ“](https://metadrivescape.github.io/papers_project/DrivePhysica/page.html)] | âœ“ | âœ— | âœ“ | 2.0 |
| **TERRA** | arXiv 2024.12 | TERRA ACT-Bench: Towards Action Controllable World Models for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2412.05337)] | [[âœ“](https://turingmotors.github.io/actbench/)] | [[âœ“](https://github.com/turingmotors/ACT-Bench)] | âœ“ | âœ“ | âœ— | 2.0 |
| **UniMLVG** | arXiv 2024.12 | UniMLVG: Unified Framework for Multi-view Long Video Generation with Comprehensive Control Capabilities | [[âœ“](https://arxiv.org/abs/2412.04842)] | [[âœ“](https://sensetime-fvg.github.io/UniMLVG/)] | [[âœ“](https://github.com/SenseTime-FVG/OpenDWM)] | âœ“ | âœ— | âœ— | 2.0 |
| **HoloDrive** | arXiv 2024.12 | HoloDrive: Holistic 2D-3D Multi-Modal Street Scene Generation for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2412.01407)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **InfinityDrive** | arXiv 2024.12 | InfinityDrive: Breaking Time Limits in Driving World Models | [[âœ“](https://arxiv.org/abs/2412.01522)] | [[âœ“](https://metadrivescape.github.io/papers_project/InfinityDrive/page.html)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Imagine-2-Drive** | arXiv 2024.11 | Imagine-2-Drive: High-Fidelity World Modeling in CARLA for Autonomous Vehicles | [[âœ“](https://arxiv.org/abs/2411.10171)] | [[âœ“](https://anantagrg.github.io/Imagine-2-Drive.github.io/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **WorldSimBench** | arXiv 2024.10 | WorldSimBench: Towards Video Generation Models as World Simulator | [[âœ“](https://arxiv.org/abs/2410.18072)] | [[âœ“](https://iranqin.github.io/WorldSimBench.github.io/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **DOME** | arXiv 2024.10 | DOME: Taming Diffusion Model into High-Fidelity Controllable Occupancy World Model | [[âœ“](https://arxiv.org/abs/2410.10429)] | [[âœ“](https://gusongen.github.io/DOME)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **OCCVAR** | OpenReview | OCCVAR: Scalable 4D Occupancy Prediction via Next-Scale Prediction | [[âœ“](https://openreview.net/forum?id=X2HnTFsFm8)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Popov et al.** | arXiv 2024.9 | Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models | [[âœ“](https://arxiv.org/abs/2409.16663)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **LatentDriver** | arXiv 2024.9 | LatentDriver: Learning Multiple Probabilistic Decisions from Latent World Model in Autonomous Driving | [[âœ“](https://arxiv.org/abs/2409.15730)] | âœ— | [[âœ“](https://github.com/Sephirex-X/LatentDriver)] | âœ“ | âœ“ | âœ— | 2.0 |
| **RenderWorld** | arXiv 2024.9 | RenderWorld: World Model with Self-Supervised 3D Label | [[âœ“](https://arxiv.org/abs/2409.11356)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **OccLLaMA** | arXiv 2024.9 | OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2409.03272)] | âœ— | [[âœ“](https://github.com/sjtuycw/OccLLaMA)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DriveGenVLM** | arXiv 2024.8 | DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving | [[âœ“](https://arxiv.org/abs/2408.16647)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Drive-OccWorld** | arXiv 2024.8 | Drive-OccWorld: Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning | [[âœ“](https://arxiv.org/abs/2408.14197)] | âœ— | [[âœ“](https://github.com/HITSZ-Automated-Driving-Lab/Drive-OccWorld)] | âœ“ | âœ“ | âœ— | 2.0 |
| **BEVWorld** | arXiv 2024.7 | BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space | [[âœ“](https://arxiv.org/abs/2407.05679)] | âœ— | [[âœ“](https://github.com/zympsyche/BevWorld)] | âœ“ | âœ“ | âœ“ | 2.0 |
| **TOKEN** | arXiv 2024.7 | Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving | [[âœ“](https://arxiv.org/abs/2407.00959)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **UMAD** | arXiv 2024.6 | UMAD: Unsupervised Mask-Level Anomaly Detection for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2406.06370)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **SimGen** | arXiv 2024.6 | SimGen: Simulator-conditioned Driving Scene Generation | [[âœ“](https://arxiv.org/abs/2406.09386)] | âœ— | [[âœ“](https://metadriverse.github.io/simgen/)] | âœ“ | âœ— | âœ— | 2.0 |
| **AdaptiveDriver** | arXiv 2024.6 | AdaptiveDriver: Planning with Adaptive World Models for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2406.10714)] | [[âœ“](https://arunbalajeev.github.io/world_models_planning/world_model_paper.html)] | [[âœ“](https://arunbalajeev.github.io/world_models_planning/world_model_paper.html)] | âœ“ | âœ“ | âœ— | 2.0 |
| **LAW** | arXiv 2024.6 | LAW: Enhancing End-to-End Autonomous Driving with Latent World Model | [[âœ“](https://arxiv.org/abs/2406.08481)] | âœ— | [[âœ“](https://github.com/BraveGroup/LAW)] | âœ“ | âœ“ | âœ— | 2.0 |
| **Delphi** | arXiv 2024.6 | Delphi: Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation | [[âœ“](https://arxiv.org/abs/2406.01349)] | âœ— | [[âœ“](https://github.com/westlake-autolab/Delphi)] | âœ“ | âœ— | âœ— | 2.0 |
| **OccSora** | arXiv 2024.5 | OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2405.20337)] | âœ— | [[âœ“](https://github.com/wzzheng/OccSora)] | âœ“ | âœ— | âœ— | 2.0 |
| **MagicDrive3D** | arXiv 2024.5 | MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes | [[âœ“](https://arxiv.org/abs/2405.14475)] | âœ— | [[âœ“](https://gaoruiyuan.com/magicdrive3d/)] | âœ“ | âœ— | âœ— | 2.0 |
| **CarDreamer** | arXiv 2024.5 | CarDreamer: Open-Source Learning Platform for World Model based Autonomous Driving | [[âœ“](https://arxiv.org/abs/2405.09111)] | âœ— | [[âœ“](https://github.com/ucd-dare/CarDreamer)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DriveSim** | arXiv 2024.5 | DriveSim: Probing Multimodal LLMs as World Models for Driving | [[âœ“](https://arxiv.org/abs/2405.05956)] | âœ— | [[âœ“](https://github.com/sreeramsa/DriveSim)] | âœ“ | âœ— | âœ— | 2.0 |
| **LidarDM** | arXiv 2024.4 | LidarDM: Generative LiDAR Simulation in a Generated World | [[âœ“](https://arxiv.org/abs/2404.02903)] | âœ— | [[âœ“](https://github.com/vzyrianov/lidardm)] | âœ“ | âœ— | âœ“ | 2.0 |
| **SubjectDrive** | arXiv 2024.3 | SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control | [[âœ“](https://arxiv.org/abs/2403.19438)] | [[âœ“](https://subjectdrive.github.io/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **DriveDreamer-2** | arXiv 2024.3 | DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation | [[âœ“](https://arxiv.org/abs/2403.06845)] | âœ— | [[âœ“](https://drivedreamer2.github.io/)] | âœ“ | âœ— | âœ“ | 2.0 |

**2025**
| Abbr. | Pub. | Full Title | Paper | Page | Code | Gene. | Plan. | Enh. | Lv. |
| :---: | :---: | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **HERMES** | ICCV 2025 | HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation | [[âœ“](https://arxiv.org/abs/2505.16394)] | [[âœ“](https://lmd0311.github.io/HERMES/)] | [[âœ“](https://github.com/LMD0311/HERMES)] | âœ“ | âœ“ | âœ— | 2.0 |
| **InfiniCube** | ICCV 2025 | InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models | [[âœ“](https://arxiv.org/abs/2412.03934)] | [[âœ“](https://research.nvidia.com/labs/toronto-ai/infinicube/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **DiST-4D** | ICCV 2025 | DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation | [[âœ“](https://arxiv.org/abs/2503.15208)] | [[âœ“](https://royalmelon0505.github.io/DiST-4D/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Epona** | ICCV 2025 | Epona: Autoregressive Diffusion World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2506.24113)] | âœ— | [[âœ“](https://github.com/Kevin-thu/Epona/)] | âœ“ | âœ“ | âœ— | 2.0 |
| **UniOcc** | ICCV 2025 | UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving | [[âœ“](https://arxiv.org/abs/2503.24381)] | âœ— | [[âœ“](https://uniocc.github.io/)] | âœ“ | âœ— | âœ— | 2.0 |
| **World4Drive** | ICCV 2025 | World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model | [[âœ“](https://arxiv.org/abs/2507.00603)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **DINO-Foresight** | NeurIPS 2025 | DINO-Foresight: Looking into the Future with DINO | [[âœ“](https://arxiv.org/abs/2412.11673)] | âœ— | [[âœ“](https://github.com/Sta8is/DINO-Foresight)] | âœ“ | âœ— | âœ— | 2.0 |
| **PolicyWM** | NeurIPS 2025 | From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction | [[âœ“](https://arxiv.org/abs/2510.19654)] | âœ— | [[âœ“](https://github.com/6550Zhao/Policy-World-Model)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DriVerse** | ACM MM 2025 | DriVerse: Navigation World Model for Driving Simulation via Multimodal Trajectory Prompting and Motion Alignment | [[âœ“](https://arxiv.org/abs/2504.19614)] | âœ— | [[âœ“](https://github.com/shalfun/DriVerse)] | âœ“ | âœ— | âœ— | 2.0 |
| **PIWM** | TIV 2025 | PIWM: Dream to Drive with Predictive Individual World Model | [[âœ“](https://arxiv.org/abs/2501.16733)] | âœ— | [[âœ“](https://github.com/gaoyinfeng/PIWM)] | âœ“ | âœ— | âœ— | 2.0 |
| **DriveDreamer4D** | CVPR 2025 | DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation | [[âœ“](https://arxiv.org/abs/2410.13571)] | [[âœ“](https://drivedreamer4d.github.io/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **GaussianWorld** | CVPR 2025 | GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction | [[âœ“](https://arxiv.org/abs/2412.10373)] | âœ— | [[âœ“](https://github.com/zuosc19/GaussianWorld)] | âœ“ | âœ— | âœ— | 2.0 |
| **ReconDreamer** | CVPR 2025 | ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration | [[âœ“](https://arxiv.org/abs/2411.19548)] | âœ— | [[âœ“](https://github.com/GigaAI-research/ReconDreamer)] | âœ“ | âœ— | âœ“ | 2.0 |
| **FUTURIST** | CVPR 2025 | FUTURIST: Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers | [[âœ“](https://arxiv.org/abs/2501.08303)] | âœ— | [[âœ“](https://github.com/Sta8is/FUTURIST)] | âœ“ | âœ— | âœ— | 2.0 |
| **MaskGWM** | CVPR 2025 | MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction | [[âœ“](https://arxiv.org/abs/2502.11663)] | âœ— | [[âœ“](https://github.com/SenseTime-FVG/OpenDWM)] | âœ“ | âœ— | âœ— | 2.0 |
| **UniScene** | CVPR 2025 | UniScene: Unified Occupancy-centric Driving Scene Generation | [[âœ“](https://arxiv.org/abs/2412.05435)] | [[âœ“](https://arlo0o.github.io/uniscene/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **GEM** | CVPR 2025 | GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control | [[âœ“](https://arxiv.org/abs/2412.11198)] | [[âœ“](https://vita-epfl.github.io/GEM.github.io/)] | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **UMGen** | CVPR 2025 | UMGen: Generating Multimodal Driving Scenes via Next-Scene Prediction | [[âœ“](https://arxiv.org/abs/2503.14945)] | [[âœ“](https://yanhaowu.github.io/UMGen/)] | [[âœ“](https://github.com/YanhaoWu/UMGen/)] | âœ“ | âœ— | âœ— | 2.0 |
| **DIO** | CVPR 2025 | DIO: Decomposable Implicit 4D Occupancy-Flow World Model | [[âœ“](https://openaccess.thecvf.com/content/CVPR2025/html/Diehl_DIO_Decomposable_Implicit_4D_Occupancy-Flow_World_Model_CVPR_2025_paper.html)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **SceneDiffuser++** | CVPR 2025 | SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model | [[âœ“](https://openaccess.thecvf.com/content/CVPR2025/html/Tan_SceneDiffuser_City-Scale_Traffic_Simulation_via_a_Generative_World_Model_CVPR_2025_paper.html)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **DynamicCity** | ICLR 2025 | DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes | [[âœ“](https://arxiv.org/abs/2410.18084)] | âœ— | [[âœ“](https://github.com/3DTopia/DynamicCity)] | âœ“ | âœ— | âœ— | 2.0 |
| **AdaWM** | ICLR 2025 | AdaWM: Adaptive World Model based Planning for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2501.13072)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **OccProphet** | ICLR 2025 | OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework | [[âœ“](https://arxiv.org/abs/2502.15180)] | âœ— | [[âœ“](https://github.com/JLChen-C/OccProphet)] | âœ“ | âœ— | âœ— | 2.0 |
| **PreWorld** | ICLR 2025 | PreWorld: Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2502.07309)] | âœ— | [[âœ“](https://github.com/getterupper/PreWorld)] | âœ“ | âœ— | âœ— | 2.0 |
| **SSR** | ICLR 2025 | SSR: Does End-to-End Autonomous Driving Really Need Perception Tasks? | [[âœ“](https://arxiv.org/abs/2409.18341)] | âœ— | [[âœ“](https://github.com/PeidongLi/SSR)] | âœ— | âœ“ | âœ— | 2.0 |
| **Occ-LLM** | ICRA 2025 | Occ-LLM: Enhancing Autonomous Driving with Occupancy-Based Large Language Models | [[âœ“](https://arxiv.org/abs/2502.06419)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **STAGE** | IROS 2025 | STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation | [[âœ“](https://arxiv.org/abs/2506.13138)] | [[âœ“](https://4dvlab.github.io/STAGE/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Drive&Gen** | IROS 2025 | Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models | [[âœ“](https://arxiv.org/abs/2510.06209)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **LiDAR-Seq** | ICCVW 2025 | Learning to Generate 4D LiDAR Sequences | [[âœ“](https://arxiv.org/abs/2509.11959)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Accident-WM** | Comms Eng 2025 | World model-based end-to-end scene generation for accident anticipation in autonomous driving | [[âœ“](https://www.nature.com/articles/s44172-025-00474-7)] | âœ— | âœ— | âœ“ | âœ— | âœ“ | 2.0 |
| **LiDAR-Nav** | JIFS 2025 | World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations | [[âœ“](https://arxiv.org/abs/2512.03429)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **MindDrive** | arXiv 2025.12 | MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving | [[âœ“](https://arxiv.org/abs/2512.04441)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **U4D** | arXiv 2025.12 | U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences | [[âœ“](https://arxiv.org/abs/2512.02982)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **ThinkByDrive** | arXiv 2025.12 | Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles | [[âœ“](https://arxiv.org/abs/2512.03454)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **VD-WM** | arXiv 2025.12 | Vehicle Dynamics Embedded World Models for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2512.02417)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **LiSTAR** | arXiv 2025.11 | LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving | [[âœ“](https://arxiv.org/abs/2511.16049)] | [[âœ“](https://ocean-luna.github.io/LiSTAR.gitub.io)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **OpenTwinMap** | arXiv 2025.11 | OpenTwinMap: An Open-Source Digital Twin Generator for Urban Autonomous Driving | [[âœ“](https://arxiv.org/abs/2511.21925)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **SparseWorld-TC** | arXiv 2025.11 | SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model | [[âœ“](https://arxiv.org/abs/2511.22039)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **LaGen** | arXiv 2025.11 | LaGen: Towards Autoregressive LiDAR Scene Generation | [[âœ“](https://arxiv.org/abs/2511.21256)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **AD-R1** | arXiv 2025.11 | AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models | [[âœ“](https://arxiv.org/abs/2511.20325)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **CorrectAD** | arXiv 2025.11 | CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving | [[âœ“](https://arxiv.org/abs/2511.13297)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **UniScenev2** | arXiv 2025.10 | UniScenev2: Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method | [[âœ“](https://arxiv.org/abs/2510.22973)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Implicit-RWM** | arXiv 2025.10 | Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models | [[âœ“](https://arxiv.org/abs/2510.16729)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **SparseWorld** | arXiv 2025.10 | SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries | [[âœ“](https://arxiv.org/abs/2510.17482)] | âœ— | [[âœ“](https://github.com/MSunDYY/SparseWorld)] | âœ“ | âœ— | âœ— | 2.0 |
| **OmniNWM** | arXiv 2025.10 | OmniNWM: Omniscient Driving Navigation World Models | [[âœ“](https://arxiv.org/abs/2510.18313)] | [[âœ“](https://arlo0o.github.io/OmniNWM/)] | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **ORAD-3D** | arXiv 2025.10 | ORAD-3D: Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks | [[âœ“](https://arxiv.org/abs/2510.16500)] | âœ— | [[âœ“](https://github.com/chaytonmin/ORAD-3D)] | âœ“ | âœ— | âœ“ | 2.0 |
| **Dream4Drive** | arXiv 2025.10 | Dream4Drive: Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks | [[âœ“](https://arxiv.org/abs/2510.19195)] | [[âœ“](https://wm-research.github.io/Dream4Drive/)] | âœ— | âœ“ | âœ— | âœ“ | 2.0 |
| **DriveVLA-W0** | arXiv 2025.10 | DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving | [[âœ“](https://arxiv.org/abs/2510.12796)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **CoIRL-AD** | arXiv 2025.10 | CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2510.12560)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **CVD-STORM** | arXiv 2025.10 | CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2510.07944)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **PhiGensis** | arXiv 2025.9 | PhiGensis: 4D Driving Scene Generation With Stereo Forcing | [[âœ“](https://arxiv.org/abs/2509.20251)] | [[âœ“](https://jiangxb98.github.io/PhiGensis/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **TeraSim-World** | arXiv 2025.9 | TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End Autonomous Driving | [[âœ“](https://arxiv.org/abs/2509.13164)] | âœ— | âœ— | âœ“ | âœ— | âœ“ | 2.0 |
| **OccTENS** | arXiv 2025.9 | OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction | [[âœ“](https://arxiv.org/abs/2509.03887)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **G^2Editor** | arXiv 2025.8 | G^2Editor: Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation | [[âœ“](https://arxiv.org/abs/2508.20471)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **LSD-3D** | arXiv 2025.8 | LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding | [[âœ“](https://arxiv.org/abs/2508.19204)] | [[âœ“](https://princeton-computational-imaging.github.io/LSD-3D/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **VideoGen-FT** | arXiv 2025.8 | Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation | [[âœ“](https://arxiv.org/abs/2508.16512)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **MoVieDrive** | arXiv 2025.8 | MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation | [[âœ“](https://arxiv.org/abs/2508.14327)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **ImagiDrive** | arXiv 2025.8 | ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2508.11428)] | âœ— | [[âœ“](https://github.com/fudan-zvg/ImagiDrive)] | âœ“ | âœ“ | âœ— | 2.0 |
| **LiDARCrafter** | arXiv 2025.8 | LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences | [[âœ“](https://arxiv.org/abs/2508.03692)] | [[âœ“](https://lidarcrafter.github.io/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **FASTopoWM** | arXiv 2025.7 | FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models | [[âœ“](https://arxiv.org/abs/2507.23325)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Accident-WM** | arXiv 2025.7 | World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving | [[âœ“](https://arxiv.org/abs/2507.12762)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Orbis** | arXiv 2025.7 | Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models | [[âœ“](https://arxiv.org/abs/2507.13162)] | âœ— | [[âœ“](https://lmb-freiburg.github.io/orbis.github.io/)] | âœ“ | âœ— | âœ— | 2.0 |
| **I2-World** | arXiv 2025.7 | IÂ²-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting | [[âœ“](https://arxiv.org/abs/2507.09144)] | âœ— | [[âœ“](https://github.com/lzzzzzm/II-World)] | âœ“ | âœ— | âœ— | 2.0 |
| **NRSeg** | arXiv 2025.7 | NRSeg: Noise-Resilient Learning for BEV Semantic Segmentation via Driving World Models | [[âœ“](https://arxiv.org/abs/2507.04002)] | âœ— | [[âœ“](https://github.com/lynn-yu/NRSeg)] | âœ“ | âœ— | âœ“ | 2.0 |
| **LiDAR-FM** | arXiv 2025.6 | Towards foundational LiDAR world models with efficient latent flow matching | [[âœ“](https://arxiv.org/abs/2506.23434)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **ReSim** | arXiv 2025.6 | ReSim: Reliable World Simulation for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2506.09981)] | [[âœ“](https://opendrivelab.com/ReSim)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Cosmos-Drive** | arXiv 2025.6 | Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models | [[âœ“](https://arxiv.org/abs/2506.09042)] | [[âœ“](https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Dreamland** | arXiv 2025.6 | Dreamland: Controllable World Creation with Simulator and Generative Models | [[âœ“](https://arxiv.org/abs/2506.08006)] | [[âœ“](https://metadriverse.github.io/dreamland/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **LongDWM** | arXiv 2025.6 | LongDWM: Cross-Granularity Distillation for Building a Long-Term Driving World Model | [[âœ“](https://arxiv.org/abs/2506.01546)] | âœ— | [[âœ“](https://wang-xiaodong1899.github.io/longdwm/)] | âœ“ | âœ“ | âœ— | 2.0 |
| **FSDrive** | arXiv 2025.5 | FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2505.17685)] | âœ— | [[âœ“](https://github.com/MIV-XJTU/FSDrive)] | âœ“ | âœ“ | âœ— | 2.0 |
| **ProphetDWM** | arXiv 2025.5 | ProphetDWM: A Driving World Model for Rolling Out Future Actions and Videos | [[âœ“](https://arxiv.org/abs/2505.18650)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **GeoDrive** | arXiv 2025.5 | GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control | [[âœ“](https://arxiv.org/abs/2505.22421)] | âœ— | [[âœ“](https://github.com/antonioo-c/GeoDrive)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DriveX** | arXiv 2025.5 | DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving | [[âœ“](https://arxiv.org/abs/2505.19239)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **VL-SAFE** | arXiv 2025.5 | VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models | [[âœ“](https://arxiv.org/abs/2505.16377)] | [[âœ“](https://ys-qu.github.io/vlsafe-website/)] | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **Raw2Drive** | arXiv 2025.5 | Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving | [[âœ“](https://arxiv.org/abs/2505.16394)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **RAMBLE** | arXiv 2025.4 | RAMBLE: From Imitation to Exploration: End-to-end Autonomous Driving based on World Model | [[âœ“](https://arxiv.org/abs/2410.02253)] | âœ— | [[âœ“](https://github.com/SCP-CN-001/ramble)] | âœ“ | âœ“ | âœ— | 2.0 |
| **DiVE** | arXiv 2025.4 | DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer | [[âœ“](https://arxiv.org/abs/2504.18576)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **WoTE** | arXiv 2025.4 | WoTE: End-to-End Driving with Online Trajectory Evaluation via BEV World Model | [[âœ“](https://arxiv.org/abs/2503.22231)] | âœ— | [[âœ“](https://github.com/liyingyanUCAS/WoTE)] | âœ“ | âœ“ | âœ— | 2.0 |
| **UniFuture** | arXiv 2025.3 | UniFuture: Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception | [[âœ“](https://arxiv.org/abs/2503.13587)] | [[âœ“](https://dk-liang.github.io/UniFuture/)] | [[âœ“](https://github.com/dk-liang/UniFuture)] | âœ“ | âœ— | âœ“ | 2.0 |
| **MagicDrive-V2** | arXiv 2025.3 | MagicDrive-V2: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control | [[âœ“](https://arxiv.org/abs/2411.13807)] | [[âœ“](https://gaoruiyuan.com/magicdrive-v2/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **CoGen** | arXiv 2025.3 | CoGen: 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2503.22231)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **GAIA-2** | arXiv 2025.3 | GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2503.20523)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **Semi-SD** | arXiv 2025.3 | Semi-SD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2503.19713)] | âœ— | [[âœ“](https://github.com/xieyuser/Semi-SD)] | âœ“ | âœ— | âœ“ | 2.0 |
| **MiLA** | arXiv 2025.3 | MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving | [[âœ“](https://arxiv.org/abs/2503.15875)] | [[âœ“](https://xiaomi-mlab.github.io/mila.github.io/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **SimWorld** | arXiv 2025.3 | SimWorld: A Unified Benchmark for Simulator-Conditioned Scene Generation via World Model | [[âœ“](https://arxiv.org/abs/2503.13952)] | âœ— | [[âœ“](https://github.com/Li-Zn-H/SimWorld)] | âœ“ | âœ— | âœ— | 2.0 |
| **EOT-WM** | arXiv 2025.3 | EOT-WM: Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories | [[âœ“](https://arxiv.org/abs/2503.09215)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **TÂ³Former** | arXiv 2025.3 | TÂ³Former: Temporal Triplane Transformers as Occupancy World Models | [[âœ“](https://arxiv.org/abs/2503.07338)] | âœ— | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **AVD2** | arXiv 2025.3 | AVD2: Accident Video Diffusion for Accident Video Description | [[âœ“](https://arxiv.org/abs/2502.14801)] | [[âœ“](https://an-answer-tree.github.io/)] | âœ— | âœ“ | âœ— | âœ— | 2.0 |
| **VaViM** | arXiv 2025.2 | VaViM and VaVAM: Autonomous Driving through Video Generative Modeling | [[âœ“](https://arxiv.org/abs/2502.15672)] | âœ— | [[âœ“](https://github.com/valeoai/VideoActionModel)] | âœ“ | âœ— | âœ— | 2.0 |
| **Dream to Drive** | arXiv 2025.2 | Dream to Drive: Model-Based Vehicle Control Using Analytic World Models | [[âœ“](https://arxiv.org/abs/2502.10012)] | âœ— | âœ— | âœ“ | âœ“ | âœ— | 2.0 |
| **AD-L-JEPA** | arXiv 2025.1 | AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture | [[âœ“](https://arxiv.org/abs/2501.04969)] | âœ— | [[âœ“](https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release)] | âœ“ | âœ— | âœ“ | 2.0 |
| **Vista** | NIPS24 | Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability | [[âœ“](https://arxiv.org/abs/2405.17398)] | âœ— | [[âœ“](https://github.com/OpenDriveLab/Vista)] | âœ“ | âœ“ | âœ— | 2.0 |

# ğŸ“„ 2. Progressive Robustness Analysis: 1.0, 2.0 and 3.0

comming soon...

## 2.1
